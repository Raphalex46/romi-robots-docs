{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"In this document you will find information on how to use and build the ROMI robots Official Website Link ROMI is an H2020 European project: Official Website . GitHub sources Link For now these sources are private. Project funding Link This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 773875.","title":"Home"},{"location":"#official-website","text":"ROMI is an H2020 European project: Official Website .","title":"Official Website"},{"location":"#github-sources","text":"For now these sources are private.","title":"GitHub sources"},{"location":"#project-funding","text":"This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 773875.","title":"Project funding"},{"location":"Cable/","text":"Wirebot Link Main Board and Eletronics Link Raspberry Pi - packages, Dependencies and Configurations Link Download the image Robotics Ubuntu+ROS Raspberry Pi Image (3B+ Support) that comes with Ubuntu 16.04 (LXDE), and ROS Kinetic. Copy the image to the SD card. Instructions here. Resizes the file system to fill the SD card before booting following this instructions. Acces to the raspi-config utility: $computer :~ $sudo raspi-config Choose \"Expand root partition to fill SD card\" option: The Ubiquityrobotics images come up as a Wifi acces point. The SSID is ubiquityrobotXXXX where XXXX is part of the MAC address. Connect to the wifi hostopost and use folowing wifi password: robotseverywhere Once connected, it is possible to log into the Pi with ssh ubuntu@10.42.0.1 with the following password of: ubuntu Desable the default robots and node runing on the pi. $ ubuntu@ubiquityrobot.local: $sudo systemctl disable magni-base Raspberry Pi - Setting up the WIREDBOT to the Network Link Open a new terminal window, and log in to the robot with ssh: ATENTION : The HOSTNAME for firts time is \u201cubiquityrobot.local\u201d. $ computer:~ $ssh ubuntu@ubiquityrobot.local ATENTION : The password for firts time is \u201cubuntu\u201d. Change the hostname using pifi. Type the following command: $ ubuntu@ubiquityrobot.local:~ $sudo pifi set-hostname wiredbot Reboot the Pi. $ ubuntu@ubiquityrobot.local:~ $sudo reboot Log in to the robot with the new hostname \"wiredbot\": $ computer:~ $ssh ubuntu@wiredbot.local Use pifi to list the nearby networks: $ ubuntu@wiredbot:~ $pifi list seen ATENTION : Search for the network where the robots are connected. Swich to to the desire network by using the following command. $ ubuntu@NEWHOSTNAME:~ $sudo pifi add localNetwork password ATTENTION : The keyword \"localNetwork\" on this documentation refert to the network the robot need to be connected. The keyword \"pass\" on this documentation refer to the password of the network. Reboot the Pi. $ ubuntu@wiredbot:~ $sudo reboot Test the connectivity with the Pi. Open a new terminal window on a external on a diferent computer: $ computer:~ $ping wirebot.local TIP : Press control-c to stop the pinging ADVERTENCE : If something goes wrong, the PI will come back up as access point mode. Search on the network for the name ubiquityrobot, reboot and start over. Log into the PI by using: $ computer:~ $ssh ubuntu@wirebot.local the output will be: The authenticity of host \u201810.0.0.113 ( 10 .0.0.113 ) \u2019 can\u2019t be established. ECDSA key fingerprint is SHA256:sDDeGZzL8FPY3kMmvhwjPC9wH+mGsAxJL/dNXpoYnsc. Are you sure you want to continue connecting ( yes/no ) ? continue by wrinting: $ computer:~ $yes the password is still. ubuntu Update and updagrade de Pi. $ ubuntu@wiredbot:~ $sudo apt-get update $ ubuntu@wiredbot:~ $sudo apt-get upgrade ROS - Setting up the ROS NODES and Arduino Firmware. Link Make sure you have installed the resent updates and updagrades. $ ubuntu@wiredbot:~ $sudo apt-get update $ ubuntu@wiredbot:~ $sudo apt-get upgrade Point to the workspace folder for ros packages Clone the repository on the Pi, the romi/grlbl_serial into the /src folder of your catkin workspace and rebuild your workspace: $ ubuntu@wiredbot:~ $cd ~/catkin_ws/src/ $ ubuntu@wiredbot:~ $git clone git@github.com:romi/grlbl_serial.git $ ubuntu@wiredbot:~ $catkin_make Clone the repository on the Pi, the romi/i2c_pca9685_driver into the /src folder of your catkin workspace and rebuild your workspace: $ ubuntu@wiredbot:~ $cd ~/catkin_ws/src/ $ ubuntu@wiredbot:~ $git clone git@github.com:romi/i2c_pca9685_driver.git $ ubuntu@wiredbot:~ $catkin_make Wiring diagram. Link Schematics: List Part Item Description Quantity 0 Raspberry pi model 3b+ 1 1 Raspberry Pi Camera Module v2 1 2 16-Channel 12-bit - I2C interface - PCA9685 1 3 Arduino UNO 1 4 Arduino CNC Shield V3 1 5 A4988 Stepper Motor Driver 4 6 Nema 23 Unipolar 1.8deg 1 7 Survey3W Camera - Orange+Cyan+NIR (OCN, NDVI) 1 8 Survey3W HDMI PWM Trigger Cable 1 9 Survey3 Advanced GPS Receiver 1 10 12V Power Supply 1 11 Wires and general hardware - Hardware Setup. Link 1.Drawings * Assebly drawing - Top view * Assebly drawing - Botton View List Part Item Description Quantity 0 Aluminium Profile 20\u00d720 T-Slot 5 4 1 Idler Pulley Plate 6 2 Join Plat T 4 3 Corner connector 90 degree (V-Slot) 2 4 Gantry Plate V-Slot 20-80 2 5 3M Drop in Tee Nuts \u2013 Insert nuts 50 6 3M Allen Low Profile Screws 50 7 M8 Allen Screw - 45mm Long 6 8 Motor Mount Plate NEMA 23 1 9 Nylon Pulley And Wheel - 40 mm Diameter - 8 mm Bearing 6 10 Nema 23 stepper motor 1 11 P65 Weatherproof Enclosure/electrical enclosure box 2 12 5mm Shock Cord - Marine Grade Polyester Coated Rubber Rope - Running ROS node - Path Planning Link ROS Nodes Overview. ROS Master - Run ROS Nodes over the raspberry PI. Log into the raspberry PI by using: $ computer:~ $ssh ubuntu@wirebot.local (OPTIONAL) Edit the path planning according to the dimensions of the field to scan and the desired length and amount of waypoints. $ ubuntu@ubiquityrobot.local:~ $sudo nano ~/catkin_ws/src/grlbl_serial/src/path_planning_action_client.py * Edit the path_planning_action_client.py by changing the variable movement_goal.xyz_position that is under the function def path_planning_client() . Here is an example of a Path planning that takes pictures of every 500mm in a distance of 10mts: movement_goal.xyz_position = [\"{'x':0, 'y':0, 'z':500, 'delay':20}\", \"{'x':0, 'y':0, 'z':1000, 'delay':20}\", \"{'x':0, 'y':0, 'z':1500, 'delay':20}\", \"{'x':0, 'y':0, 'z':2000, 'delay':20}\", \"{'x':0, 'y':0, 'z':2500, 'delay':20}\", \"{'x':0, 'y':0, 'z':3000, 'delay':20}\", \"{'x':0, 'y':0, 'z':3500, 'delay':20}\", \"{'x':0, 'y':0, 'z':4000, 'delay':20}\", \"{'x':0, 'y':0, 'z':4500, 'delay':20}\", \"{'x':0, 'y':0, 'z':5000, 'delay':20}\", \"{'x':0, 'y':0, 'z':5500, 'delay':20}\", \"{'x':0, 'y':0, 'z':6000, 'delay':20}\", \"{'x':0, 'y':0, 'z':6500, 'delay':20}\", \"{'x':0, 'y':0, 'z':7000, 'delay':20}\", \"{'x':0, 'y':0, 'z':7500, 'delay':20}\", \"{'x':0, 'y':0, 'z':8000, 'delay':20}\", \"{'x':0, 'y':0, 'z':8500, 'delay':20}\", \"{'x':0, 'y':0, 'z':9000, 'delay':20}\", \"{'x':0, 'y':0, 'z':9500, 'delay':20}\"] Start up the nodes and the ROS master by launching the path_planning_action_server_node node under the raspberry PI: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_server_node.launch (ADVERTENCE) If the ROS package is not under the autocomplete method of the terminal. The problem will be solve by sourcing the devel/setup.bash. $ ubuntu@ubiquityrobot.local:~ $source ~/catkin_ws/src/devel/setup.bash 3. ROS Slave - Run ROS Nodes over the Remote Computer. Start up the nodes by launching the path_planning_action_client_node node under the remote computer: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_client_node.launch (OPTIONAL) This node as well can by launch over the raspberry PI. This can be done by lauching the node over a new terminal. By launching the previous ROS node on the WIREDBOT. The starting process of collecting photos from the Mapir camera and the Raspi Cam will be launch automatically according to the path planning instructions save on the path_planning_action_client.py file. Saving the data from the WIREDBOT. (UNDER-DEVELOPMENT) Link Kepp running or re start the node and the ROS master by launching the path_planning_action_server_node node under the raspberry PI: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_server_node.launch Publish a 1500us to the /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues mapir_control_pwm: $ ubuntu@ubiquityrobot.local:~ $rostopic pub -1 /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues int16 mapir_control_pwm 1500 int16 motor_A_pwm int16 motor_B_pwm Once ros is publishing the message mapir_control_pwm 1500us under the topic \\i2c_pca9685_driver\\wiredbot_PWMValues. The camera is ready to mount. On the raspberry PI. Mount the camera by using the following commands. $ ubuntu@ubiquityrobot.local:~ $mkdir /mapir $ ubuntu@ubiquityrobot.local:~ $mkdir sudo mount -t vfat /dev/sdb2 /mapir $ ubuntu@ubiquityrobot.local:~ $cd /mapir/DCMI/Photos * Image Gallery - Valldaura: Link Lettuce Think and Wirebot: Wirebot on the field: Wirebot on the field: WIREBOT 3D Scans: Link","title":"Index"},{"location":"Cable/#wirebot","text":"","title":"Wirebot"},{"location":"Cable/#main-board-and-eletronics","text":"","title":"Main Board and Eletronics"},{"location":"Cable/#raspberry-pi-packages-dependencies-and-configurations","text":"Download the image Robotics Ubuntu+ROS Raspberry Pi Image (3B+ Support) that comes with Ubuntu 16.04 (LXDE), and ROS Kinetic. Copy the image to the SD card. Instructions here. Resizes the file system to fill the SD card before booting following this instructions. Acces to the raspi-config utility: $computer :~ $sudo raspi-config Choose \"Expand root partition to fill SD card\" option: The Ubiquityrobotics images come up as a Wifi acces point. The SSID is ubiquityrobotXXXX where XXXX is part of the MAC address. Connect to the wifi hostopost and use folowing wifi password: robotseverywhere Once connected, it is possible to log into the Pi with ssh ubuntu@10.42.0.1 with the following password of: ubuntu Desable the default robots and node runing on the pi. $ ubuntu@ubiquityrobot.local: $sudo systemctl disable magni-base","title":"Raspberry Pi - packages, Dependencies and Configurations"},{"location":"Cable/#raspberry-pi-setting-up-the-wiredbot-to-the-network","text":"Open a new terminal window, and log in to the robot with ssh: ATENTION : The HOSTNAME for firts time is \u201cubiquityrobot.local\u201d. $ computer:~ $ssh ubuntu@ubiquityrobot.local ATENTION : The password for firts time is \u201cubuntu\u201d. Change the hostname using pifi. Type the following command: $ ubuntu@ubiquityrobot.local:~ $sudo pifi set-hostname wiredbot Reboot the Pi. $ ubuntu@ubiquityrobot.local:~ $sudo reboot Log in to the robot with the new hostname \"wiredbot\": $ computer:~ $ssh ubuntu@wiredbot.local Use pifi to list the nearby networks: $ ubuntu@wiredbot:~ $pifi list seen ATENTION : Search for the network where the robots are connected. Swich to to the desire network by using the following command. $ ubuntu@NEWHOSTNAME:~ $sudo pifi add localNetwork password ATTENTION : The keyword \"localNetwork\" on this documentation refert to the network the robot need to be connected. The keyword \"pass\" on this documentation refer to the password of the network. Reboot the Pi. $ ubuntu@wiredbot:~ $sudo reboot Test the connectivity with the Pi. Open a new terminal window on a external on a diferent computer: $ computer:~ $ping wirebot.local TIP : Press control-c to stop the pinging ADVERTENCE : If something goes wrong, the PI will come back up as access point mode. Search on the network for the name ubiquityrobot, reboot and start over. Log into the PI by using: $ computer:~ $ssh ubuntu@wirebot.local the output will be: The authenticity of host \u201810.0.0.113 ( 10 .0.0.113 ) \u2019 can\u2019t be established. ECDSA key fingerprint is SHA256:sDDeGZzL8FPY3kMmvhwjPC9wH+mGsAxJL/dNXpoYnsc. Are you sure you want to continue connecting ( yes/no ) ? continue by wrinting: $ computer:~ $yes the password is still. ubuntu Update and updagrade de Pi. $ ubuntu@wiredbot:~ $sudo apt-get update $ ubuntu@wiredbot:~ $sudo apt-get upgrade","title":"Raspberry Pi - Setting up the WIREDBOT to the Network"},{"location":"Cable/#ros-setting-up-the-ros-nodes-and-arduino-firmware","text":"Make sure you have installed the resent updates and updagrades. $ ubuntu@wiredbot:~ $sudo apt-get update $ ubuntu@wiredbot:~ $sudo apt-get upgrade Point to the workspace folder for ros packages Clone the repository on the Pi, the romi/grlbl_serial into the /src folder of your catkin workspace and rebuild your workspace: $ ubuntu@wiredbot:~ $cd ~/catkin_ws/src/ $ ubuntu@wiredbot:~ $git clone git@github.com:romi/grlbl_serial.git $ ubuntu@wiredbot:~ $catkin_make Clone the repository on the Pi, the romi/i2c_pca9685_driver into the /src folder of your catkin workspace and rebuild your workspace: $ ubuntu@wiredbot:~ $cd ~/catkin_ws/src/ $ ubuntu@wiredbot:~ $git clone git@github.com:romi/i2c_pca9685_driver.git $ ubuntu@wiredbot:~ $catkin_make","title":"ROS - Setting up the ROS NODES and Arduino Firmware."},{"location":"Cable/#wiring-diagram","text":"Schematics: List Part Item Description Quantity 0 Raspberry pi model 3b+ 1 1 Raspberry Pi Camera Module v2 1 2 16-Channel 12-bit - I2C interface - PCA9685 1 3 Arduino UNO 1 4 Arduino CNC Shield V3 1 5 A4988 Stepper Motor Driver 4 6 Nema 23 Unipolar 1.8deg 1 7 Survey3W Camera - Orange+Cyan+NIR (OCN, NDVI) 1 8 Survey3W HDMI PWM Trigger Cable 1 9 Survey3 Advanced GPS Receiver 1 10 12V Power Supply 1 11 Wires and general hardware -","title":"Wiring diagram."},{"location":"Cable/#hardware-setup","text":"1.Drawings * Assebly drawing - Top view * Assebly drawing - Botton View List Part Item Description Quantity 0 Aluminium Profile 20\u00d720 T-Slot 5 4 1 Idler Pulley Plate 6 2 Join Plat T 4 3 Corner connector 90 degree (V-Slot) 2 4 Gantry Plate V-Slot 20-80 2 5 3M Drop in Tee Nuts \u2013 Insert nuts 50 6 3M Allen Low Profile Screws 50 7 M8 Allen Screw - 45mm Long 6 8 Motor Mount Plate NEMA 23 1 9 Nylon Pulley And Wheel - 40 mm Diameter - 8 mm Bearing 6 10 Nema 23 stepper motor 1 11 P65 Weatherproof Enclosure/electrical enclosure box 2 12 5mm Shock Cord - Marine Grade Polyester Coated Rubber Rope -","title":"Hardware Setup."},{"location":"Cable/#running-ros-node-path-planning","text":"ROS Nodes Overview. ROS Master - Run ROS Nodes over the raspberry PI. Log into the raspberry PI by using: $ computer:~ $ssh ubuntu@wirebot.local (OPTIONAL) Edit the path planning according to the dimensions of the field to scan and the desired length and amount of waypoints. $ ubuntu@ubiquityrobot.local:~ $sudo nano ~/catkin_ws/src/grlbl_serial/src/path_planning_action_client.py * Edit the path_planning_action_client.py by changing the variable movement_goal.xyz_position that is under the function def path_planning_client() . Here is an example of a Path planning that takes pictures of every 500mm in a distance of 10mts: movement_goal.xyz_position = [\"{'x':0, 'y':0, 'z':500, 'delay':20}\", \"{'x':0, 'y':0, 'z':1000, 'delay':20}\", \"{'x':0, 'y':0, 'z':1500, 'delay':20}\", \"{'x':0, 'y':0, 'z':2000, 'delay':20}\", \"{'x':0, 'y':0, 'z':2500, 'delay':20}\", \"{'x':0, 'y':0, 'z':3000, 'delay':20}\", \"{'x':0, 'y':0, 'z':3500, 'delay':20}\", \"{'x':0, 'y':0, 'z':4000, 'delay':20}\", \"{'x':0, 'y':0, 'z':4500, 'delay':20}\", \"{'x':0, 'y':0, 'z':5000, 'delay':20}\", \"{'x':0, 'y':0, 'z':5500, 'delay':20}\", \"{'x':0, 'y':0, 'z':6000, 'delay':20}\", \"{'x':0, 'y':0, 'z':6500, 'delay':20}\", \"{'x':0, 'y':0, 'z':7000, 'delay':20}\", \"{'x':0, 'y':0, 'z':7500, 'delay':20}\", \"{'x':0, 'y':0, 'z':8000, 'delay':20}\", \"{'x':0, 'y':0, 'z':8500, 'delay':20}\", \"{'x':0, 'y':0, 'z':9000, 'delay':20}\", \"{'x':0, 'y':0, 'z':9500, 'delay':20}\"] Start up the nodes and the ROS master by launching the path_planning_action_server_node node under the raspberry PI: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_server_node.launch (ADVERTENCE) If the ROS package is not under the autocomplete method of the terminal. The problem will be solve by sourcing the devel/setup.bash. $ ubuntu@ubiquityrobot.local:~ $source ~/catkin_ws/src/devel/setup.bash 3. ROS Slave - Run ROS Nodes over the Remote Computer. Start up the nodes by launching the path_planning_action_client_node node under the remote computer: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_client_node.launch (OPTIONAL) This node as well can by launch over the raspberry PI. This can be done by lauching the node over a new terminal. By launching the previous ROS node on the WIREDBOT. The starting process of collecting photos from the Mapir camera and the Raspi Cam will be launch automatically according to the path planning instructions save on the path_planning_action_client.py file.","title":"Running ROS node - Path Planning"},{"location":"Cable/#saving-the-data-from-the-wiredbot-under-development","text":"Kepp running or re start the node and the ROS master by launching the path_planning_action_server_node node under the raspberry PI: $ ubuntu@ubiquityrobot.local:~ $roslaunch grlbl_serial path_planning_action_server_node.launch Publish a 1500us to the /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues mapir_control_pwm: $ ubuntu@ubiquityrobot.local:~ $rostopic pub -1 /i2c_pca9685_driver wiredbot_PWMValues/wiredbot_PWMValues int16 mapir_control_pwm 1500 int16 motor_A_pwm int16 motor_B_pwm Once ros is publishing the message mapir_control_pwm 1500us under the topic \\i2c_pca9685_driver\\wiredbot_PWMValues. The camera is ready to mount. On the raspberry PI. Mount the camera by using the following commands. $ ubuntu@ubiquityrobot.local:~ $mkdir /mapir $ ubuntu@ubiquityrobot.local:~ $mkdir sudo mount -t vfat /dev/sdb2 /mapir $ ubuntu@ubiquityrobot.local:~ $cd /mapir/DCMI/Photos *","title":"Saving the data from the WIREDBOT. (UNDER-DEVELOPMENT)"},{"location":"Cable/#image-gallery-valldaura","text":"Lettuce Think and Wirebot: Wirebot on the field: Wirebot on the field:","title":"Image Gallery - Valldaura:"},{"location":"Cable/#wirebot-3d-scans","text":"","title":"WIREBOT 3D Scans:"},{"location":"Rover/","text":"In this document you will find information on how to use and build the ROMI Rover... User Manual Hardware Documentation Software Installation Developer Documentation","title":"Index"},{"location":"Rover/developer/","text":"Developer Documentation Link","title":"Developer"},{"location":"Rover/developer/#developer-documentation","text":"","title":"Developer Documentation"},{"location":"Rover/hardware/","text":"Hardware Documentation Link This document describes the hardware, both the mechanical parts and the electronics.. The main structure Link The figure below gives an overview of the main components. The mechanical components Link The frame Link The wheels Link The boxes Link The CNC Link We use currently use the X-Carve. Please follow X-Carve's documentation at . The Z-axis Link The cover Link The electronics Link The basic architecture of the control modules Link NOTE : The current rovers don't implements the schema above, yet: The CNC has no encoders. The rover in Valldaur doesn't have a control panel. The control panel Link NOTE : The control panel is still being developed at the time of writing. The control panel is used to start and stop the rover and to display status information on the character display. Component Specifications Example Controller Arduino Uno or equivalent Proto shield The shield allows you to solder the wires securely Adafruit Sparkfun Amazon Relay (2x) TODO Sparkfun Push button with LEDS (2x) One green and one red Adafruit LCD Character Display Comptable with XXX Farnell The power circuit Link NOTE : This is the new power circuit that will be used with the control panel. It's currently not implemented in either rover. There are three separate power circuits: Always-on circuit : This circuits powers the control panel. Logic circuit : This circuits powers the embedded PC and other control circuits. Power circuit : This circuit drives all the motors. This is the circuit that is cut when the security switch (the big red button) is pressed. The control panel actuates two relays (Relay 1 & 2) according to the two start-up phases (the PC and the logic circuits start up first, then the motors are powered up). The third relay is designed to handle strong currents. It has a protection against sparks and back-currents. Most of the logic runs on 5V. (TODO: Add a Meanwell power converter. Q: One converter for the the control panel + one of the logic circuit? Or one single converter?) The figure does not show the power line for the weeding hoe. The hoe is turned on/off using a fourth relay that is connected to the gShield board of the CNC. It using the CNC's spindle on/off functionality. You can find more information on this in the section on the CNC below. Component Specifications Example Relay Non-Latching, protection against sparks and back-current (TODO: be more precise) RS Online Security switch Farnell: button and housing The navigation module Link The navigation uses a differential wheel drive, with two motorized wheels in the back and two swivel caster in the front. This makes the control fairly straight-forward and the components are easy to source. The navigation module can also receive input from a remote control to steer the rover. The main components are shown below: Components Link Component Specifications Example Motors Brushed DC motors , 24 V, minimum 200 W We are using wheel chair motors for now. We bought a set at Superdroid Robots . Encoders Incremental encoders US Digital E2 (Available from Superdroid Robotics ). Controller Arduino Uno or equivalent Proto shield The shield allows you to solder the wires securely Adafruit Sparkfun Amazon Motor driver Preferably one board that can drive two motors. Two drivers, one for each motor, is possible, too. Power input: 24V, Maximum output current: > 15 A per motor, Control signals: two PWM signals (similar to RC input) for the left and right motor. The driver implements a standard H-bridge to control to power supplied to the motors (both forward and backward rotation). Sabertooth 2x60A Sabertooth 2x32A RoboClaw 2x60A RC controller and receiver A standard RC receiver that outputs a PWM signal. Powered by 5V. We've succesfully used the remote controllers for Spektrum , this one for example or similar Wheels You will need those, too. We are using the wheel provided by Superdroid Robotics for now Wiring diagram Link The tool positioning Link A CNC is adapted for use in the rover. We replaced the spindle that is normally used to carve wooden pieces, with a rotating weeding hoe. We are using to larger, 1000 mm sized version of the X-Carve . The newer X-Carve uses a custom design board for the control. However, we prefer using the older solution that combines an Arduino Uno with a gShield because it is smaller and more generic. For the time being, we use the grbl language to send commands from the embedded PC to the CNC controller. Therefore, any solution that accepts grbl commands should be drop-in solution for the Uno+gShield combo. Component Specifications Example CNC Minimum work area: 0.7 m x 0.7m X-Carve Optional: Controller board Must run grbl interpreter Arduino Uno Optional: Stepper drivers (3 steppers) The driver must use the STEP/DIR control signals gShield [Arduino CNC Shield] eBay and RepRap You can still have a look at XCarve's older documentation on how to wire the controller boards: http://x-carve-instructions.inventables.com/xcarve2015/step10/ http://x-carve-instructions.inventables.com/xcarve2015/step14/ Notable, the following two diagrams are of interest: The yellow wire marked \"spinle\" in the image above is used to turn the weeding hoe on or off, as shown in the figure below (see also the figure in the section on the power circuit).","title":"Hardware"},{"location":"Rover/hardware/#hardware-documentation","text":"This document describes the hardware, both the mechanical parts and the electronics..","title":"Hardware Documentation"},{"location":"Rover/hardware/#the-main-structure","text":"The figure below gives an overview of the main components.","title":"The main structure"},{"location":"Rover/hardware/#the-mechanical-components","text":"","title":"The mechanical components"},{"location":"Rover/hardware/#the-frame","text":"","title":"The frame"},{"location":"Rover/hardware/#the-wheels","text":"","title":"The wheels"},{"location":"Rover/hardware/#the-boxes","text":"","title":"The boxes"},{"location":"Rover/hardware/#the-cnc","text":"We use currently use the X-Carve. Please follow X-Carve's documentation at .","title":"The CNC"},{"location":"Rover/hardware/#the-z-axis","text":"","title":"The Z-axis"},{"location":"Rover/hardware/#the-cover","text":"","title":"The cover"},{"location":"Rover/hardware/#the-electronics","text":"","title":"The electronics"},{"location":"Rover/hardware/#the-basic-architecture-of-the-control-modules","text":"NOTE : The current rovers don't implements the schema above, yet: The CNC has no encoders. The rover in Valldaur doesn't have a control panel.","title":"The basic architecture of the control modules"},{"location":"Rover/hardware/#the-control-panel","text":"NOTE : The control panel is still being developed at the time of writing. The control panel is used to start and stop the rover and to display status information on the character display. Component Specifications Example Controller Arduino Uno or equivalent Proto shield The shield allows you to solder the wires securely Adafruit Sparkfun Amazon Relay (2x) TODO Sparkfun Push button with LEDS (2x) One green and one red Adafruit LCD Character Display Comptable with XXX Farnell","title":"The control panel"},{"location":"Rover/hardware/#the-power-circuit","text":"NOTE : This is the new power circuit that will be used with the control panel. It's currently not implemented in either rover. There are three separate power circuits: Always-on circuit : This circuits powers the control panel. Logic circuit : This circuits powers the embedded PC and other control circuits. Power circuit : This circuit drives all the motors. This is the circuit that is cut when the security switch (the big red button) is pressed. The control panel actuates two relays (Relay 1 & 2) according to the two start-up phases (the PC and the logic circuits start up first, then the motors are powered up). The third relay is designed to handle strong currents. It has a protection against sparks and back-currents. Most of the logic runs on 5V. (TODO: Add a Meanwell power converter. Q: One converter for the the control panel + one of the logic circuit? Or one single converter?) The figure does not show the power line for the weeding hoe. The hoe is turned on/off using a fourth relay that is connected to the gShield board of the CNC. It using the CNC's spindle on/off functionality. You can find more information on this in the section on the CNC below. Component Specifications Example Relay Non-Latching, protection against sparks and back-current (TODO: be more precise) RS Online Security switch Farnell: button and housing","title":"The power circuit"},{"location":"Rover/hardware/#the-navigation-module","text":"The navigation uses a differential wheel drive, with two motorized wheels in the back and two swivel caster in the front. This makes the control fairly straight-forward and the components are easy to source. The navigation module can also receive input from a remote control to steer the rover. The main components are shown below:","title":"The navigation module"},{"location":"Rover/hardware/#components","text":"Component Specifications Example Motors Brushed DC motors , 24 V, minimum 200 W We are using wheel chair motors for now. We bought a set at Superdroid Robots . Encoders Incremental encoders US Digital E2 (Available from Superdroid Robotics ). Controller Arduino Uno or equivalent Proto shield The shield allows you to solder the wires securely Adafruit Sparkfun Amazon Motor driver Preferably one board that can drive two motors. Two drivers, one for each motor, is possible, too. Power input: 24V, Maximum output current: > 15 A per motor, Control signals: two PWM signals (similar to RC input) for the left and right motor. The driver implements a standard H-bridge to control to power supplied to the motors (both forward and backward rotation). Sabertooth 2x60A Sabertooth 2x32A RoboClaw 2x60A RC controller and receiver A standard RC receiver that outputs a PWM signal. Powered by 5V. We've succesfully used the remote controllers for Spektrum , this one for example or similar Wheels You will need those, too. We are using the wheel provided by Superdroid Robotics for now","title":"Components"},{"location":"Rover/hardware/#wiring-diagram","text":"","title":"Wiring diagram"},{"location":"Rover/hardware/#the-tool-positioning","text":"A CNC is adapted for use in the rover. We replaced the spindle that is normally used to carve wooden pieces, with a rotating weeding hoe. We are using to larger, 1000 mm sized version of the X-Carve . The newer X-Carve uses a custom design board for the control. However, we prefer using the older solution that combines an Arduino Uno with a gShield because it is smaller and more generic. For the time being, we use the grbl language to send commands from the embedded PC to the CNC controller. Therefore, any solution that accepts grbl commands should be drop-in solution for the Uno+gShield combo. Component Specifications Example CNC Minimum work area: 0.7 m x 0.7m X-Carve Optional: Controller board Must run grbl interpreter Arduino Uno Optional: Stepper drivers (3 steppers) The driver must use the STEP/DIR control signals gShield [Arduino CNC Shield] eBay and RepRap You can still have a look at XCarve's older documentation on how to wire the controller boards: http://x-carve-instructions.inventables.com/xcarve2015/step10/ http://x-carve-instructions.inventables.com/xcarve2015/step14/ Notable, the following two diagrams are of interest: The yellow wire marked \"spinle\" in the image above is used to turn the weeding hoe on or off, as shown in the figure below (see also the figure in the section on the power circuit).","title":"The tool positioning"},{"location":"Rover/manual/","text":"TODO: Overview Preparing the beds Main components of the rover Charging the battery Turning the rover on/off The web interface Configuration Calibration Starting a program Modifying a program Adding a program Assistance Troubleshooting","title":"User Manual"},{"location":"Rover/software/","text":"Software Installation Link Overview Link This document describes how to run and compile the software for the ROMI Rover. If you are a developer looking for details on the source code then have a look at the separate Developer Documentation . Prerequisites Link The software of the rover runs on Linux. It is not tied to a specific Linux distribution but we have tested it mostly on recent versions of Debian (includin Raspian) and Ubuntu. The software is mostly writen in C and depends on the following libraries: libr : Common code for the rcom and the libromi libraries. It provides some OS abstraction (for example for threads, memory allocation, file system, networking), some core functionality (logging, time), and some base classes (variable-size memory buffers, json parser, lists, serial connections). Code rcom : An inter-process communication framework. It provides real-time communication using UDP messages and high-level communication based on web protocols (HTTP, Websockets). It also includes several utilities to develop and manage rcom applications. Code libromi : Base classes for the romi rover: fsdb (database with filesystem back-end), image loading and manipulations, \u2026) Code romi-brush-motor-controller : The motor controller. Code romi-rover : All of the apps for the Romi rover. Code By default, the rover uses a USB camera. It is possible to use the Intel Realsense camera on the Picamera instead. In that case, you will have to install additional libraries (see XXX). Installing a Raspberry Pi from scratch Link We use the Lite version of Raspbian. You can download it at https://www.raspberrypi.org/downloads/raspbian/ . There are several ways to prepare the disk image for the RPi. Check the page at https://www.raspberrypi.org/documentation/installation/installing-images/ (there\u2019s lots of information available on this topic online) and follow the instructions that suit you best. Once you have the SD card, connect RPi to screen, keyboard and network (ethernet), power up the board and log in (user pi , password raspberry ). The first thing you want to do is change some of the default settings using the raspi-config tool. In the console type: $ sudo raspi-config The list of settings that you may want to look at includes: 1 Change User Password 2 Network Options Hostname WiFi 4 Localisation Options Change locales Change keyboard layout 5 Interfacing Options Enable Camera Enable SSH 8 Update Next, create the user \u2018romi\u2019: $ sudo adduser romi $ sudo adduser romi dialout $ sudo adduser romi video $ sudo adduser romi sudo After that, quit the current session and login again as user \u2018romi\u2019. The nano text editor is installed by default but if you prefer anoher editor, now is a good time to install it: $ sudo apt install emacs-nox ( ... or any editor you like : ) Install the developer tools: $ sudo apt install build-essential cmake git Install the software dependencies: $ sudo apt install libpng-dev libjpeg9-dev That's it. You should be ready. Quick install #!/bin/bash # Install the dependencies sudo apt install build-essential cmake git libpng-dev libjpeg9-dev # Download, compile and install the libraries & apps for id in libr rcom libromi romi-rover ; do echo ---------------------------------------------- echo Compiling $id # Download or update the github repository if [ -d $id ] ; then cd $id git pull else git clone https://github.com/romi/ $id .git cd $id fi # Standard cmake build sequence mkdir -p build cd build cmake .. make sudo make install sudo ldconfig # Get ready for the next component cd ../.. done Installing the romi-rover apps Link You should first install the libr , rcom , and libromi libraries. Check out their the Github pages for the installation instruction and the API documentation. You should also flash the motor controller to the Arduino (instructions are available on Github ) too. Once that is done, the installation of the romi-rover apps is straight-forward. First, check out the code: $ git clone https://github.com/romi/romi-rover.git Then proceed to the compilation and installation: $ cd romi-rover $ mkdir build $ cd build $ cmake .. $ make $ sudo make install Compiling the picamera app Link Although not currently used by the ROMI Rover, we have an Rcom app to access the Picamera. To get it working, you will first have to install the raspicam library: $ git clone https://github.com/cedricve/raspicam.git $ cd raspicam/ $ mkdir build $ cd build/ $ cmake .. $ make $ sudo make install $ sudo ldconfig Once raspicam is installed, you must re-run cmake to enable the compilation of the picamera app: $ cd romi-rover/build/ $ rm CMakeCache.txt $ cmake .. -DWITH_PICAMERA = ON $ make $ sudo make install Compiling the realsense app Link The fonctionality of the Realsense camera app is not complete. You can use it to obtain RGB images and depth images (as BW PNG images). You will first have to install librealsense2 . When librealsense is installed, re-run cmake to enable the compilation of the realsense app: $ cd romi-rover/build/ $ rm CMakeCache.txt $ cmake .. -DWITH_REALSENSE = ON $ make $ sudo make install Configuration Link Configuring the romi-rover apps Link In the directory /home/romi, create the following directories and copy the default configuration and script files: $ cd /home/romi $ mkdir sessions $ mkdir config $ cp <romi-rover>/config/config-romi-rover.json config/ $ mkdir scripts $ cp <romi-rover>/script/config-default.json scripts/ \"html\": \"<romi-rover>/interface/html\", Starting the apps on boot Link Currently we are still using the old rc.local mechanism. The file /etc/rc.local is no longer included in more recent Ubuntu versions. If ls /etc/rc.local returns an error, you will have to create the file as follows: $ sudo nano /etc/rc.local Copy the following contents: #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"exit 0\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. exit 0 Finally, make the script executable. $ sudo chmod +x /etc/rc.local To enable the apps on start-up, add the following line in /etc/rc.local, above the exit 0 line: /usr/local/bin/rclaunch /home/romi/config/config-romi-rover.json & Configuring the image uploads Link Annex: the apps and their options Link \"fake_camera\": { \"image\": \"data/camera.jpg\" },","title":"Software"},{"location":"Rover/software/#software-installation","text":"","title":"Software Installation"},{"location":"Rover/software/#overview","text":"This document describes how to run and compile the software for the ROMI Rover. If you are a developer looking for details on the source code then have a look at the separate Developer Documentation .","title":"Overview"},{"location":"Rover/software/#prerequisites","text":"The software of the rover runs on Linux. It is not tied to a specific Linux distribution but we have tested it mostly on recent versions of Debian (includin Raspian) and Ubuntu. The software is mostly writen in C and depends on the following libraries: libr : Common code for the rcom and the libromi libraries. It provides some OS abstraction (for example for threads, memory allocation, file system, networking), some core functionality (logging, time), and some base classes (variable-size memory buffers, json parser, lists, serial connections). Code rcom : An inter-process communication framework. It provides real-time communication using UDP messages and high-level communication based on web protocols (HTTP, Websockets). It also includes several utilities to develop and manage rcom applications. Code libromi : Base classes for the romi rover: fsdb (database with filesystem back-end), image loading and manipulations, \u2026) Code romi-brush-motor-controller : The motor controller. Code romi-rover : All of the apps for the Romi rover. Code By default, the rover uses a USB camera. It is possible to use the Intel Realsense camera on the Picamera instead. In that case, you will have to install additional libraries (see XXX).","title":"Prerequisites"},{"location":"Rover/software/#installing-a-raspberry-pi-from-scratch","text":"We use the Lite version of Raspbian. You can download it at https://www.raspberrypi.org/downloads/raspbian/ . There are several ways to prepare the disk image for the RPi. Check the page at https://www.raspberrypi.org/documentation/installation/installing-images/ (there\u2019s lots of information available on this topic online) and follow the instructions that suit you best. Once you have the SD card, connect RPi to screen, keyboard and network (ethernet), power up the board and log in (user pi , password raspberry ). The first thing you want to do is change some of the default settings using the raspi-config tool. In the console type: $ sudo raspi-config The list of settings that you may want to look at includes: 1 Change User Password 2 Network Options Hostname WiFi 4 Localisation Options Change locales Change keyboard layout 5 Interfacing Options Enable Camera Enable SSH 8 Update Next, create the user \u2018romi\u2019: $ sudo adduser romi $ sudo adduser romi dialout $ sudo adduser romi video $ sudo adduser romi sudo After that, quit the current session and login again as user \u2018romi\u2019. The nano text editor is installed by default but if you prefer anoher editor, now is a good time to install it: $ sudo apt install emacs-nox ( ... or any editor you like : ) Install the developer tools: $ sudo apt install build-essential cmake git Install the software dependencies: $ sudo apt install libpng-dev libjpeg9-dev That's it. You should be ready. Quick install #!/bin/bash # Install the dependencies sudo apt install build-essential cmake git libpng-dev libjpeg9-dev # Download, compile and install the libraries & apps for id in libr rcom libromi romi-rover ; do echo ---------------------------------------------- echo Compiling $id # Download or update the github repository if [ -d $id ] ; then cd $id git pull else git clone https://github.com/romi/ $id .git cd $id fi # Standard cmake build sequence mkdir -p build cd build cmake .. make sudo make install sudo ldconfig # Get ready for the next component cd ../.. done","title":"Installing a Raspberry Pi from scratch"},{"location":"Rover/software/#installing-the-romi-rover-apps","text":"You should first install the libr , rcom , and libromi libraries. Check out their the Github pages for the installation instruction and the API documentation. You should also flash the motor controller to the Arduino (instructions are available on Github ) too. Once that is done, the installation of the romi-rover apps is straight-forward. First, check out the code: $ git clone https://github.com/romi/romi-rover.git Then proceed to the compilation and installation: $ cd romi-rover $ mkdir build $ cd build $ cmake .. $ make $ sudo make install","title":"Installing the romi-rover apps"},{"location":"Rover/software/#compiling-the-picamera-app","text":"Although not currently used by the ROMI Rover, we have an Rcom app to access the Picamera. To get it working, you will first have to install the raspicam library: $ git clone https://github.com/cedricve/raspicam.git $ cd raspicam/ $ mkdir build $ cd build/ $ cmake .. $ make $ sudo make install $ sudo ldconfig Once raspicam is installed, you must re-run cmake to enable the compilation of the picamera app: $ cd romi-rover/build/ $ rm CMakeCache.txt $ cmake .. -DWITH_PICAMERA = ON $ make $ sudo make install","title":"Compiling the picamera app"},{"location":"Rover/software/#compiling-the-realsense-app","text":"The fonctionality of the Realsense camera app is not complete. You can use it to obtain RGB images and depth images (as BW PNG images). You will first have to install librealsense2 . When librealsense is installed, re-run cmake to enable the compilation of the realsense app: $ cd romi-rover/build/ $ rm CMakeCache.txt $ cmake .. -DWITH_REALSENSE = ON $ make $ sudo make install","title":"Compiling the realsense app"},{"location":"Rover/software/#configuration","text":"","title":"Configuration"},{"location":"Rover/software/#configuring-the-romi-rover-apps","text":"In the directory /home/romi, create the following directories and copy the default configuration and script files: $ cd /home/romi $ mkdir sessions $ mkdir config $ cp <romi-rover>/config/config-romi-rover.json config/ $ mkdir scripts $ cp <romi-rover>/script/config-default.json scripts/ \"html\": \"<romi-rover>/interface/html\",","title":"Configuring the romi-rover apps"},{"location":"Rover/software/#starting-the-apps-on-boot","text":"Currently we are still using the old rc.local mechanism. The file /etc/rc.local is no longer included in more recent Ubuntu versions. If ls /etc/rc.local returns an error, you will have to create the file as follows: $ sudo nano /etc/rc.local Copy the following contents: #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"exit 0\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. exit 0 Finally, make the script executable. $ sudo chmod +x /etc/rc.local To enable the apps on start-up, add the following line in /etc/rc.local, above the exit 0 line: /usr/local/bin/rclaunch /home/romi/config/config-romi-rover.json &","title":"Starting the apps on boot"},{"location":"Rover/software/#configuring-the-image-uploads","text":"","title":"Configuring the image uploads"},{"location":"Rover/software/#annex-the-apps-and-their-options","text":"\"fake_camera\": { \"image\": \"data/camera.jpg\" },","title":"Annex: the apps and their options"},{"location":"Scanner/data/","text":"This page describe how to use the romi package data-storage accessible here . A shared example datasets is accessible here . Getting started Link Installation Link Warning If you intend to contribute to the development of data-storage or want to be able to edit the code and test your changes, you should choose editable mode . Note This uses ssh and thus requires to be registered as part of the project and to deploy ssh keys. Editable mode Link Clone from github and install using pip : git clone https://github.com/romi/data-storage.git cd data-storage pip install -e . Basic working example Link Assume you have a list of images you want to create a database, and add some images to a scan in this database. 1 - Initialize database Link First create the folder for the database and add the romidb marker to it: from os.path import join from tempfile import mkdtemp mydb = mkdtemp ( prefix = 'romidb_' ) open ( join ( mydb , 'romidb' ), 'w' ) . close () Npw you can initialize a ROMI FSDB database object: from romidata.fsdb import FSDB db = FSDB ( mydb ) db . connect () # Locks the database and allows access 2 - Create a new datasets Link To create a new datasets, here named myscan_001 , do: scan = db . create_scan ( \"myscan_001\" ) To add scan metadata ( eg. camera settings, biological metadata, hardware metadata...), do: scan . set_metadata ({ \"scanner\" : { \"harware\" : 'test' }}) This will results in several changes in the local database: Add a myscan_001 sub-folder in the database root folder; Add a metadata sub-folder in myscan_001 and a metadata.json gathering the given scan metadata . 3 - Add images as new fileset Link OPTIONAL - create a list of RGB images If you do not have a scan datasets available, either download a shared datasets here or generate a list of images as follows: import numpy as np # Generate random noise images n_images = 99 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) Create a new fileset : fileset = scan . create_fileset ( \"images\" ) Add the images to the fileset: Load the file list (or skip if you generated random images): from os import listdir imgs = listdir ( \"</path/to/my/scan/images>\" ) Then loop the images list and add them to the fileset , optionally attach some metadata to each image: from romidata import io for i , img in enumerate ( imgs ): file = fileset . create_file ( \" %i \" % i ) io . write_image ( file , img ) file . set_metadata ( \"key\" , \" %i \" % i ) This will results in several changes in the local database: Reference the image by its file name by adding an entry in files.json ; Write a scan_img_1.jpeg image in the images sub-folder of the scan \"myscan\" . Add an images sub-folder in the metadata sub-folder, and JSON files with the image id as name to store the image metadata . 4 - Access image files in a fileset Link To access the image files in a fileset (in a datasets, itself in an existing and accessible database), procced as follows: from romidata.fsdb import FSDB db = FSDB ( mydb ) db . connect () # Locks the database and allows access scan = db . get_scan ( \"myscan\" ) fileset = scan . get_fileset ( \"images\" ) for f in fileset . get_files (): im = io . read_image ( f ) # reads image data print ( f . get_metadata ( \"key\" )) # i db . disconnect () Examples Link from romidata.fsdb import FSDB from romidata import io import numpy as np # Generate random noise images n_images = 100 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) from os import listdir from os.path import join from tempfile import mkdtemp # Create a temporary DB folder: mydb = mkdtemp ( prefix = 'romidb_' ) # Create the `romidb` file in previously created temporary DB folder: open ( join ( mydb , 'romidb' ), 'w' ) . close () listdir ( mydb ) # Connect to the DB: db = FSDB ( mydb ) db . connect () # Locks the database and allows access # Add a scan datasets to the DB: scan = db . create_scan ( \"myscan_001\" ) listdir ( mydb ) # Add metadata to a scan datasets: scan . set_metadata ({ \"scanner\" : { \"harware\" : 'test' }}) listdir ( join ( mydb , \"myscan_001\" )) listdir ( join ( mydb , \"myscan_001\" , \"metadata\" )) fileset = scan . create_fileset ( \"images\" ) listdir ( join ( mydb , \"myscan_001\" )) for i , img in enumerate ( imgs ): file = fileset . create_file ( \" %i \" % i ) io . write_image ( file , img ) file . set_metadata ( \"key\" , \" %i \" % i ) # Add some metadata # read files in the fileset: scan = db . get_scan ( \"myscan\" ) fileset = scan . get_fileset ( \"images\" ) for f in fileset . get_files (): im = io . read_image ( f ) # reads image data print ( f . get_metadata ( \"key\" )) # i db . disconnect () Folders structure Link Database root folder Link A root database folder is defined, eg. mydb . Inside this folder we need to defines (add) the romidb marker so it may be used by fsdb . We may also find the lock file used to limit the access to the database to only one user. Note that this part is manual, you have to create these manually: mkdir mydb touch mydb/romidb Once you have created this root folder and file, you can initialize a ROMI FSDB database object: from romidata.fsdb import FSDB db = FSDB ( \"mydb\" ) db . connect () The method FSDB.connect() locks the database with a lock file at root directory and allows access. You may remove the lock file if you are sure no one else is using the database. Within this root database folder you will find other folders corresponding to datasets . Datasets folders Link At the next level, we find the datasets folder(s), eg. named myscan_001 . Their names must be uniques and you create them as follow: scan = db . create_scan ( \"myscan_001\" ) If you add scan metadata ( eg. camera settings, biological metadata, hardware metadata...) with scan.set_metadata() , you get another folder metadata with a metadata.json file. We now have the following tree structure: mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 romidb Images folders Link Inside myscan_001/ , we find the datasets or fileset in romidb terminology. In the case of the \"plant scanner\", this is a list of RGB image files acquired by a camera moving around the plant. To store the datasets, we thus name the created fileset \"images\": fileset = scan . create_fileset ( \"images\" ) Inside this images/ folder will reside the images added to the database. At the same time you added images with REF_TO_TUTO , you created an entry in a JSON file referencing the files. If you added metadata along with the files ( eg. camera poses, jpeg metadata...) it should be referenced in metadata/images/ eg. metadata/images/<scan_img_01>.json . mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u2502 \u251c\u2500\u2500 scan_img_01.jpg \u2502 \u2502 \u251c\u2500\u2500 scan_img_02.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 scan_img_99.jpg \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 scan_img_01.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 scan_img_02.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 scan_img_99.json \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 romidb Example Link mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a \u2502 \u2502 \u2514\u2500\u2500 AnglesAndInternodes.json \u2502 \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413 \u2502 \u2502 \u251c\u2500\u2500 cameras.json \u2502 \u2502 \u251c\u2500\u2500 images.json \u2502 \u2502 \u251c\u2500\u2500 points3d.json \u2502 \u2502 \u2514\u2500\u2500 sparse.ply \u2502 \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20 \u2502 \u2502 \u2514\u2500\u2500 CurveSkeleton.json \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.jpg \u2502 \u251c\u2500\u2500 Masks_True_5_out_9adb9db801 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.jpg \u2502 \u251c\u2500\u2500 measures.csv \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a.json \u2502 \u2502 \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413.json \u2502 \u2502 \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20.json \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 images.json \u2502 \u2502 \u251c\u2500\u2500 Masks_True_5_out_9adb9db801 \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 Masks_True_5_out_e90d1804eb.json \u2502 \u2502 \u251c\u2500\u2500 metadata.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b \u2502 \u2502 \u2502 \u2514\u2500\u2500 PointCloud.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud__200_0_1_0_False_4ce2e46446.json \u2502 \u2502 \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821.json \u2502 \u2502 \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81.json \u2502 \u2502 \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____False_567dc7f48b \u2502 \u2502 \u2502 \u2514\u2500\u2500 Voxels.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____False_567dc7f48b.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____True_af037e876e.json \u2502 \u2502 \u2514\u2500\u2500 Voxels_False____True_cd9a5ff06b.json \u2502 \u251c\u2500\u2500 pipeline.toml \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b \u2502 \u2502 \u2514\u2500\u2500 PointCloud.ply \u2502 \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821 \u2502 \u2502 \u2514\u2500\u2500 TreeGraph.p \u2502 \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81 \u2502 \u2502 \u2514\u2500\u2500 TriangleMesh.ply \u2502 \u2514\u2500\u2500 Voxels_False____False_567dc7f48b \u2502 \u2514\u2500\u2500 Voxels.npz \u251c\u2500\u2500 colmap_log.txt \u251c\u2500\u2500 lock \u2514\u2500\u2500 romidb","title":"Data"},{"location":"Scanner/data/#getting-started","text":"","title":"Getting started"},{"location":"Scanner/data/#installation","text":"Warning If you intend to contribute to the development of data-storage or want to be able to edit the code and test your changes, you should choose editable mode . Note This uses ssh and thus requires to be registered as part of the project and to deploy ssh keys.","title":"Installation"},{"location":"Scanner/data/#editable-mode","text":"Clone from github and install using pip : git clone https://github.com/romi/data-storage.git cd data-storage pip install -e .","title":"Editable mode"},{"location":"Scanner/data/#basic-working-example","text":"Assume you have a list of images you want to create a database, and add some images to a scan in this database.","title":"Basic working example"},{"location":"Scanner/data/#1-initialize-database","text":"First create the folder for the database and add the romidb marker to it: from os.path import join from tempfile import mkdtemp mydb = mkdtemp ( prefix = 'romidb_' ) open ( join ( mydb , 'romidb' ), 'w' ) . close () Npw you can initialize a ROMI FSDB database object: from romidata.fsdb import FSDB db = FSDB ( mydb ) db . connect () # Locks the database and allows access","title":"1 - Initialize database"},{"location":"Scanner/data/#2-create-a-new-datasets","text":"To create a new datasets, here named myscan_001 , do: scan = db . create_scan ( \"myscan_001\" ) To add scan metadata ( eg. camera settings, biological metadata, hardware metadata...), do: scan . set_metadata ({ \"scanner\" : { \"harware\" : 'test' }}) This will results in several changes in the local database: Add a myscan_001 sub-folder in the database root folder; Add a metadata sub-folder in myscan_001 and a metadata.json gathering the given scan metadata .","title":"2 - Create a new datasets"},{"location":"Scanner/data/#3-add-images-as-new-fileset","text":"OPTIONAL - create a list of RGB images If you do not have a scan datasets available, either download a shared datasets here or generate a list of images as follows: import numpy as np # Generate random noise images n_images = 99 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) Create a new fileset : fileset = scan . create_fileset ( \"images\" ) Add the images to the fileset: Load the file list (or skip if you generated random images): from os import listdir imgs = listdir ( \"</path/to/my/scan/images>\" ) Then loop the images list and add them to the fileset , optionally attach some metadata to each image: from romidata import io for i , img in enumerate ( imgs ): file = fileset . create_file ( \" %i \" % i ) io . write_image ( file , img ) file . set_metadata ( \"key\" , \" %i \" % i ) This will results in several changes in the local database: Reference the image by its file name by adding an entry in files.json ; Write a scan_img_1.jpeg image in the images sub-folder of the scan \"myscan\" . Add an images sub-folder in the metadata sub-folder, and JSON files with the image id as name to store the image metadata .","title":"3 - Add images as new fileset"},{"location":"Scanner/data/#4-access-image-files-in-a-fileset","text":"To access the image files in a fileset (in a datasets, itself in an existing and accessible database), procced as follows: from romidata.fsdb import FSDB db = FSDB ( mydb ) db . connect () # Locks the database and allows access scan = db . get_scan ( \"myscan\" ) fileset = scan . get_fileset ( \"images\" ) for f in fileset . get_files (): im = io . read_image ( f ) # reads image data print ( f . get_metadata ( \"key\" )) # i db . disconnect ()","title":"4 - Access image files in a fileset"},{"location":"Scanner/data/#examples","text":"from romidata.fsdb import FSDB from romidata import io import numpy as np # Generate random noise images n_images = 100 imgs = [] for i in range ( n_images ): img = 256 * np . random . rand ( 256 , 256 , 3 ) img = np . array ( img , dtype = np . uint8 ) imgs . append ( img ) from os import listdir from os.path import join from tempfile import mkdtemp # Create a temporary DB folder: mydb = mkdtemp ( prefix = 'romidb_' ) # Create the `romidb` file in previously created temporary DB folder: open ( join ( mydb , 'romidb' ), 'w' ) . close () listdir ( mydb ) # Connect to the DB: db = FSDB ( mydb ) db . connect () # Locks the database and allows access # Add a scan datasets to the DB: scan = db . create_scan ( \"myscan_001\" ) listdir ( mydb ) # Add metadata to a scan datasets: scan . set_metadata ({ \"scanner\" : { \"harware\" : 'test' }}) listdir ( join ( mydb , \"myscan_001\" )) listdir ( join ( mydb , \"myscan_001\" , \"metadata\" )) fileset = scan . create_fileset ( \"images\" ) listdir ( join ( mydb , \"myscan_001\" )) for i , img in enumerate ( imgs ): file = fileset . create_file ( \" %i \" % i ) io . write_image ( file , img ) file . set_metadata ( \"key\" , \" %i \" % i ) # Add some metadata # read files in the fileset: scan = db . get_scan ( \"myscan\" ) fileset = scan . get_fileset ( \"images\" ) for f in fileset . get_files (): im = io . read_image ( f ) # reads image data print ( f . get_metadata ( \"key\" )) # i db . disconnect ()","title":"Examples"},{"location":"Scanner/data/#folders-structure","text":"","title":"Folders structure"},{"location":"Scanner/data/#database-root-folder","text":"A root database folder is defined, eg. mydb . Inside this folder we need to defines (add) the romidb marker so it may be used by fsdb . We may also find the lock file used to limit the access to the database to only one user. Note that this part is manual, you have to create these manually: mkdir mydb touch mydb/romidb Once you have created this root folder and file, you can initialize a ROMI FSDB database object: from romidata.fsdb import FSDB db = FSDB ( \"mydb\" ) db . connect () The method FSDB.connect() locks the database with a lock file at root directory and allows access. You may remove the lock file if you are sure no one else is using the database. Within this root database folder you will find other folders corresponding to datasets .","title":"Database root folder"},{"location":"Scanner/data/#datasets-folders","text":"At the next level, we find the datasets folder(s), eg. named myscan_001 . Their names must be uniques and you create them as follow: scan = db . create_scan ( \"myscan_001\" ) If you add scan metadata ( eg. camera settings, biological metadata, hardware metadata...) with scan.set_metadata() , you get another folder metadata with a metadata.json file. We now have the following tree structure: mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 romidb","title":"Datasets folders"},{"location":"Scanner/data/#images-folders","text":"Inside myscan_001/ , we find the datasets or fileset in romidb terminology. In the case of the \"plant scanner\", this is a list of RGB image files acquired by a camera moving around the plant. To store the datasets, we thus name the created fileset \"images\": fileset = scan . create_fileset ( \"images\" ) Inside this images/ folder will reside the images added to the database. At the same time you added images with REF_TO_TUTO , you created an entry in a JSON file referencing the files. If you added metadata along with the files ( eg. camera poses, jpeg metadata...) it should be referenced in metadata/images/ eg. metadata/images/<scan_img_01>.json . mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images/ \u2502 \u2502 \u251c\u2500\u2500 scan_img_01.jpg \u2502 \u2502 \u251c\u2500\u2500 scan_img_02.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 scan_img_99.jpg \u2502 \u251c\u2500\u2500 metadata/ \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 scan_img_01.json \u2502 \u2502 \u2502 \u251c\u2500\u2500 scan_img_02.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 scan_img_99.json \u2502 \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 romidb","title":"Images folders"},{"location":"Scanner/data/#example","text":"mydb/ \u251c\u2500\u2500 myscan_001/ \u2502 \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a \u2502 \u2502 \u2514\u2500\u2500 AnglesAndInternodes.json \u2502 \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413 \u2502 \u2502 \u251c\u2500\u2500 cameras.json \u2502 \u2502 \u251c\u2500\u2500 images.json \u2502 \u2502 \u251c\u2500\u2500 points3d.json \u2502 \u2502 \u2514\u2500\u2500 sparse.ply \u2502 \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20 \u2502 \u2502 \u2514\u2500\u2500 CurveSkeleton.json \u2502 \u251c\u2500\u2500 files.json \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.jpg \u2502 \u251c\u2500\u2500 Masks_True_5_out_9adb9db801 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.jpg \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.jpg \u2502 \u251c\u2500\u2500 measures.csv \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u251c\u2500\u2500 AnglesAndInternodes_1_0_2_0_0_1_dd8d67653a.json \u2502 \u2502 \u251c\u2500\u2500 Colmap_True____feature_extrac_3bbfcb1413.json \u2502 \u2502 \u251c\u2500\u2500 CurveSkeleton_out__TriangleMesh_6a92751c20.json \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 images.json \u2502 \u2502 \u251c\u2500\u2500 Masks_True_5_out_9adb9db801 \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 Masks_True_5_out_e90d1804eb.json \u2502 \u2502 \u251c\u2500\u2500 metadata.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b \u2502 \u2502 \u2502 \u2514\u2500\u2500 PointCloud.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b.json \u2502 \u2502 \u251c\u2500\u2500 PointCloud__200_0_1_0_False_4ce2e46446.json \u2502 \u2502 \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821.json \u2502 \u2502 \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81.json \u2502 \u2502 \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff \u2502 \u2502 \u2502 \u251c\u2500\u2500 pict20190201_110110_0.json \u2502 \u2502 \u251c\u2500\u2500 [...] \u2502 \u2502 \u2502 \u2514\u2500\u2500 pict20190201_111209_0.json \u2502 \u2502 \u251c\u2500\u2500 Undistorted_out_____fb3e3fa0ff.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____False_567dc7f48b \u2502 \u2502 \u2502 \u2514\u2500\u2500 Voxels.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____False_567dc7f48b.json \u2502 \u2502 \u251c\u2500\u2500 Voxels_False____True_af037e876e.json \u2502 \u2502 \u2514\u2500\u2500 Voxels_False____True_cd9a5ff06b.json \u2502 \u251c\u2500\u2500 pipeline.toml \u2502 \u251c\u2500\u2500 PointCloud_1_0_1_0_False_9ab5a15d9b \u2502 \u2502 \u2514\u2500\u2500 PointCloud.ply \u2502 \u251c\u2500\u2500 TreeGraph_out__CurveSkeleton_5dca9a2821 \u2502 \u2502 \u2514\u2500\u2500 TreeGraph.p \u2502 \u251c\u2500\u2500 TriangleMesh_out__PointCloud_80dc94ac81 \u2502 \u2502 \u2514\u2500\u2500 TriangleMesh.ply \u2502 \u2514\u2500\u2500 Voxels_False____False_567dc7f48b \u2502 \u2514\u2500\u2500 Voxels.npz \u251c\u2500\u2500 colmap_log.txt \u251c\u2500\u2500 lock \u2514\u2500\u2500 romidb","title":"Example"},{"location":"Scanner/hardware/","text":"Hardware setup and instructions Link Network overview Link We start by introducing the general network design of the ROMI Plant Phenotyper: Hardware configuration files Link To gather configuration information of the hardware we use toml files to define variables. This allows for easy import in python. For example, saving the following lines in a config.toml : [Scan.scanner] camera_firmware = \"sony_wifi\" cnc_firmware = \"grbl-v1.1\" gimbal_firmware = \"blgimbal\" In python: >>> import toml >>> conf = toml . load ( open ( 'config.toml' )) >>> print ( conf ) { 'Scan' : { 'scanner' : { 'camera_firmware' : 'sony_wifi' , 'cnc_firmware' : 'grbl-v1.1' , 'gimbal_firmware' : 'blgimbal' }}} >>> print ( conf [ \"Scan\" ][ \"scanner\" ][ \"camera_firmware\" ]) \"sony_wifi\" PiZero camera rovercam Link WORK IN PROGRESS!!!!! Configuring the access point host software (hostapd) Link Source: Raspberry Foundation website . 1. General setup Link Switch over to systemd-networkd : # deinstall classic networking sudo apt --autoremove purge ifupdown dhcpcd5 isc-dhcp-client isc-dhcp-common rm -r /etc/network /etc/dhcp # enable systemd-networkd systemctl enable systemd-networkd.service # setup systemd-resolved systemctl enable systemd-resolved.service apt --autoremove purge avahi-daemon apt install libnss-resolve ln -sf /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf 2. Configure wpa_supplicant as access point Link To configure wpa_supplicant as access point create this file with your settings for country= , ssid= , psk= and maybe frequency= . You can just copy and paste this in one block to your command line beginning with cat and including both EOF (delimiter EOF will not get part of the file): cat > / etc / wpa_supplicant / wpa_supplicant - wlan0 . conf << EOF country = DE ctrl_interface = DIR =/ var / run / wpa_supplicant GROUP = netdev update_config = 1 network = { ssid = \"RPiNet\" mode = 2 frequency = 2437 # key_mgmt = NONE # uncomment this for an open hotspot # delete next 3 lines if key_mgmt = NONE key_mgmt = WPA - PSK proto = RSN WPA psk = \"password\" } EOF chmod 600 /etc/wpa_supplicant/wpa_supplicant-wlan0.conf systemctl disable wpa_supplicant.service systemctl enable wpa_supplicant@wlan0.service Setting up a stand alone access point Link Example for this setup: wifi mobile-phone <~.~.~.~.~> ( wlan0 ) RPi ( eth0 ) \\ / ( dhcp ) 192 .168.4.1 Do \"General setup\" then create the following file to configure wlan0 . We only have the access point. There is no ethernet device configured. cat > /etc/systemd/network/08-wlan0.network <<EOF [Match] Name=wlan0 [Network] Address=192.168.4.1/24 MulticastDNS=yes DHCPServer=yes EOF If you want this then reboot. That's it. Otherwise go on, no need to reboot at this time.","title":"Hardware"},{"location":"Scanner/hardware/#hardware-setup-and-instructions","text":"","title":"Hardware setup and instructions"},{"location":"Scanner/hardware/#network-overview","text":"We start by introducing the general network design of the ROMI Plant Phenotyper:","title":"Network overview"},{"location":"Scanner/hardware/#hardware-configuration-files","text":"To gather configuration information of the hardware we use toml files to define variables. This allows for easy import in python. For example, saving the following lines in a config.toml : [Scan.scanner] camera_firmware = \"sony_wifi\" cnc_firmware = \"grbl-v1.1\" gimbal_firmware = \"blgimbal\" In python: >>> import toml >>> conf = toml . load ( open ( 'config.toml' )) >>> print ( conf ) { 'Scan' : { 'scanner' : { 'camera_firmware' : 'sony_wifi' , 'cnc_firmware' : 'grbl-v1.1' , 'gimbal_firmware' : 'blgimbal' }}} >>> print ( conf [ \"Scan\" ][ \"scanner\" ][ \"camera_firmware\" ]) \"sony_wifi\"","title":"Hardware configuration files"},{"location":"Scanner/hardware/#pizero-camera-rovercam","text":"WORK IN PROGRESS!!!!!","title":"PiZero camera rovercam"},{"location":"Scanner/hardware/#configuring-the-access-point-host-software-hostapd","text":"Source: Raspberry Foundation website .","title":"Configuring the access point host software (hostapd)"},{"location":"Scanner/hardware/#1-general-setup","text":"Switch over to systemd-networkd : # deinstall classic networking sudo apt --autoremove purge ifupdown dhcpcd5 isc-dhcp-client isc-dhcp-common rm -r /etc/network /etc/dhcp # enable systemd-networkd systemctl enable systemd-networkd.service # setup systemd-resolved systemctl enable systemd-resolved.service apt --autoremove purge avahi-daemon apt install libnss-resolve ln -sf /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf","title":"1. General setup"},{"location":"Scanner/hardware/#2-configure-wpa_supplicant-as-access-point","text":"To configure wpa_supplicant as access point create this file with your settings for country= , ssid= , psk= and maybe frequency= . You can just copy and paste this in one block to your command line beginning with cat and including both EOF (delimiter EOF will not get part of the file): cat > / etc / wpa_supplicant / wpa_supplicant - wlan0 . conf << EOF country = DE ctrl_interface = DIR =/ var / run / wpa_supplicant GROUP = netdev update_config = 1 network = { ssid = \"RPiNet\" mode = 2 frequency = 2437 # key_mgmt = NONE # uncomment this for an open hotspot # delete next 3 lines if key_mgmt = NONE key_mgmt = WPA - PSK proto = RSN WPA psk = \"password\" } EOF chmod 600 /etc/wpa_supplicant/wpa_supplicant-wlan0.conf systemctl disable wpa_supplicant.service systemctl enable wpa_supplicant@wlan0.service","title":"2. Configure wpa_supplicant as access point"},{"location":"Scanner/hardware/#setting-up-a-stand-alone-access-point","text":"Example for this setup: wifi mobile-phone <~.~.~.~.~> ( wlan0 ) RPi ( eth0 ) \\ / ( dhcp ) 192 .168.4.1 Do \"General setup\" then create the following file to configure wlan0 . We only have the access point. There is no ethernet device configured. cat > /etc/systemd/network/08-wlan0.network <<EOF [Match] Name=wlan0 [Network] Address=192.168.4.1/24 MulticastDNS=yes DHCPServer=yes EOF If you want this then reboot. That's it. Otherwise go on, no need to reboot at this time.","title":"Setting up a stand alone access point"},{"location":"Scanner/home/","text":"Plant scanner overview Link User story Link The user put his/her plant inside the scanner and run acquisitions , which returns a set of images per plant. These images are uploaded to a central database . The user defines a pipeline to reconstruct and quantify plants architecture by choosing among a set of predefined methods and algorithms. These instructions may be run by a distant server. Finally the user can access the acquisitions, reconstructions & quantitative data by connecting to a visualization server using his/her computer Modular architecture Link As we don't want to always have to install every repository from the ROMI project, we need to properly organize our code to make it modular. For example, there is no point to install the lettucethink-python package on a server used to perform plant reconstruction tasks and that is not \"attached\" to the RPS. Modular design Link The following figure shows each independent module and the way they interact. Module details Link Each of the following modules should be seen as separate virtual machines or containers able to communicates. Validated Module Name Container Name ROMI Packages DB romi_db data-storage PlantScanner romi_scan lettucethink-python , romiscanner SmartInterpreter romi_interpreter Scan3D , romiseg Visualizer romi_viz 3d-plantviewer VirtualScanner romi_virtual VirtualPlants , romiscanner Info The previous module container or package names, are still open to discussion! DB Link Should be totally independent of the rest since it could be use with other projects trough the abstract class DB or even the local database class FSDB . PlantScanner Link It requires a physical connection to the hardware that is should control and needs a database to export acquired datasets (plant images). SmartInterpreter Link It requires connection to a database to import datasets to process and export results. VirtualScanner Link It requires a connection to a database to export generated datasets (virtual plant images). In case of machine learning methods, a database would also provides training datasets. Visualizer Link It requires a database with datasets to browse and represent. Semantics Link We hereafter defines the semantic, names and abbreviations to use in the projects documentations and communications. Macroscopic & non-technical Link ROMI Softwares : the whole set of software developed by ROMI; ROMI Hardwares : the three types of robots developed by ROMI, namely the \"cable bot\", the \"rover\" and the \"scanner\"; ROMI Scanner Softwares - the set of software developed by the \"scanner group\"; ROMI Plant Scanner - RPS: the hardware that enable (automatic) acquisition of a set of 2D or 3D images of the plant; (Single) Plant Reconstruction Pipeline - (S)PRP: the set of methods (and packages?) used to performs a 3D reconstruction of a plant using data from the ROMI Plant Scanner ; Plant Phenotyping Pipeline - PPP or P 3 : the set of methods (and packages?) used to performs plant phenotyping (traits quantification) from the obtained 3D reconstruction; Virtual Plant Image Generator - VPIG: the set of methods (and packages?) used to generate sets of images in a similar fashion than the RPS; Repository & packages Link Validated Repository Package Description data-storage romidata The database API used in the ROMI project, as well as classes for data processing using luigi. Scan3D romiscan This repo gathers the elements used to run 3D scan of individual plants by ROMI partners. romiscanner romiscanner Hardware interface for the 3D Scanner, as well as virtual scanner lettucethink-python lettucethink-python Python tools and controllers for the lettucethink robot Segmentation romiseg Virtual plant segmentation methods using 2D images generated from the virtual scanner and neural networks. 3d-plantviewer ? Browser application to visualize 3D scanned plants VirtualPlants ? Models of various plants in LPy Info The previous names, repository or package, is still open to discussion! Database related Link database: the database itself; datasets/project: a set of images and the pipelines results; fileset: a set of files ( eg. a set of RGB images of a plant); plant metadata: set of FAIR metadata attached to the plant ( eg. species, age, growth conditions...); acquisition metadata: set of metadata attached to the acquisition procedure & hardware configuration ( eg. version of the CNC controller, camera settings, ...); Danger datasets/project are called \"scan\" for now!","title":"Home"},{"location":"Scanner/home/#plant-scanner-overview","text":"","title":"Plant scanner overview"},{"location":"Scanner/home/#user-story","text":"The user put his/her plant inside the scanner and run acquisitions , which returns a set of images per plant. These images are uploaded to a central database . The user defines a pipeline to reconstruct and quantify plants architecture by choosing among a set of predefined methods and algorithms. These instructions may be run by a distant server. Finally the user can access the acquisitions, reconstructions & quantitative data by connecting to a visualization server using his/her computer","title":"User story"},{"location":"Scanner/home/#modular-architecture","text":"As we don't want to always have to install every repository from the ROMI project, we need to properly organize our code to make it modular. For example, there is no point to install the lettucethink-python package on a server used to perform plant reconstruction tasks and that is not \"attached\" to the RPS.","title":"Modular architecture"},{"location":"Scanner/home/#modular-design","text":"The following figure shows each independent module and the way they interact.","title":"Modular design"},{"location":"Scanner/home/#module-details","text":"Each of the following modules should be seen as separate virtual machines or containers able to communicates. Validated Module Name Container Name ROMI Packages DB romi_db data-storage PlantScanner romi_scan lettucethink-python , romiscanner SmartInterpreter romi_interpreter Scan3D , romiseg Visualizer romi_viz 3d-plantviewer VirtualScanner romi_virtual VirtualPlants , romiscanner Info The previous module container or package names, are still open to discussion!","title":"Module details"},{"location":"Scanner/home/#db","text":"Should be totally independent of the rest since it could be use with other projects trough the abstract class DB or even the local database class FSDB .","title":"DB"},{"location":"Scanner/home/#plantscanner","text":"It requires a physical connection to the hardware that is should control and needs a database to export acquired datasets (plant images).","title":"PlantScanner"},{"location":"Scanner/home/#smartinterpreter","text":"It requires connection to a database to import datasets to process and export results.","title":"SmartInterpreter"},{"location":"Scanner/home/#virtualscanner","text":"It requires a connection to a database to export generated datasets (virtual plant images). In case of machine learning methods, a database would also provides training datasets.","title":"VirtualScanner"},{"location":"Scanner/home/#visualizer","text":"It requires a database with datasets to browse and represent.","title":"Visualizer"},{"location":"Scanner/home/#semantics","text":"We hereafter defines the semantic, names and abbreviations to use in the projects documentations and communications.","title":"Semantics"},{"location":"Scanner/home/#macroscopic-non-technical","text":"ROMI Softwares : the whole set of software developed by ROMI; ROMI Hardwares : the three types of robots developed by ROMI, namely the \"cable bot\", the \"rover\" and the \"scanner\"; ROMI Scanner Softwares - the set of software developed by the \"scanner group\"; ROMI Plant Scanner - RPS: the hardware that enable (automatic) acquisition of a set of 2D or 3D images of the plant; (Single) Plant Reconstruction Pipeline - (S)PRP: the set of methods (and packages?) used to performs a 3D reconstruction of a plant using data from the ROMI Plant Scanner ; Plant Phenotyping Pipeline - PPP or P 3 : the set of methods (and packages?) used to performs plant phenotyping (traits quantification) from the obtained 3D reconstruction; Virtual Plant Image Generator - VPIG: the set of methods (and packages?) used to generate sets of images in a similar fashion than the RPS;","title":"Macroscopic &amp; non-technical"},{"location":"Scanner/home/#repository-packages","text":"Validated Repository Package Description data-storage romidata The database API used in the ROMI project, as well as classes for data processing using luigi. Scan3D romiscan This repo gathers the elements used to run 3D scan of individual plants by ROMI partners. romiscanner romiscanner Hardware interface for the 3D Scanner, as well as virtual scanner lettucethink-python lettucethink-python Python tools and controllers for the lettucethink robot Segmentation romiseg Virtual plant segmentation methods using 2D images generated from the virtual scanner and neural networks. 3d-plantviewer ? Browser application to visualize 3D scanned plants VirtualPlants ? Models of various plants in LPy Info The previous names, repository or package, is still open to discussion!","title":"Repository &amp; packages"},{"location":"Scanner/home/#database-related","text":"database: the database itself; datasets/project: a set of images and the pipelines results; fileset: a set of files ( eg. a set of RGB images of a plant); plant metadata: set of FAIR metadata attached to the plant ( eg. species, age, growth conditions...); acquisition metadata: set of metadata attached to the acquisition procedure & hardware configuration ( eg. version of the CNC controller, camera settings, ...); Danger datasets/project are called \"scan\" for now!","title":"Database related"},{"location":"Scanner/pipelines/","text":"Warning This is a work in progress... the original author has no idea what he is doing! Legend Link dfd2 params Input parameters task Task name module description. task_output Task output (EXT) Note shaped boxes are RomiConfig , they are TOML files that contains parameters for each task. Round shaped boxes are RomiTasks with their name on the first level, then the module names ( --module option in romi_run_task ) and a quick description of the tasks at hand. Folder shaped boxes are RomiTarget , they indicate files input/output and the file extension is given between parenthesis. Real plant datasets Link dfd2 cluster_level1 Real plants input Scanner parameters (TOML) scan_task Scan romiscanner.scan Scan a plant (requires romi scanner). input->scan_task scan_out1 Multiple RGB images (PNG|JPEG) scan_task->scan_out1 scan_out2 Approximate camera poses from CNC (JSON) scan_task->scan_out2 scan_out3 Metadata (JSON) scan_task->scan_out3 Virtual plant datasets Link dfd2 cluster_level1 Virtual plant generator lpy_input LPY parameters (TOML) virtualplant_task VirtualPlant romiscanner.lpy LPY 3D plant generator. lpy_input->virtualplant_task vs_input VirtualScan parameters (TOML) virtualscan_task VirtualScan romiscanner.scan Generate photorealistic images of a virtual plant. vs_input->virtualscan_task virtualplant_out1 3D Plant (OBJ) virtualplant_task->virtualplant_out1 virtualplant_out2 Angles & internodes (JSON) virtualplant_task->virtualplant_out2 virtualplant_out1->virtualscan_task virtualscan_out1 Multiple RGB images (PNG|JPEG) virtualscan_task->virtualscan_out1 virtualscan_out2 Camera poses (JSON) virtualscan_task->virtualscan_out2 Plant Reconstruction from RGB images Link dfd2 cluster_level0 Algorithmic Pipeline cluster_level1 Machine Learning Pipeline input Multiple RGB images (PNG|JPEG) undistorted_task Undistorted romiscan.tasks.proc2d Undistorts images using computed intrinsic camera parameters. input->undistorted_task colmap_task Colmap romiscan.tasks.colmap Camera poses estimation. input->colmap_task mask_task Mask romiscan.tasks.proc2d 'Plant pixels' detection. input->mask_task undistorted_out Undistorted images (PNG) undistorted_task->undistorted_out undistorted_out->mask_task segmentation_task Segmentation2D romiscan.tasks.proc2d Compute masks using trained deep learning models. undistorted_out->segmentation_task colmap_out_img_md Images metadata (JSON) images.json colmap_task->colmap_out_img_md colmap_out_cam Camera poses (JSON) cameras.json colmap_task->colmap_out_cam colmap_out_pts Points in 3D (JSON) points3D.json colmap_task->colmap_out_pts colmap_out_ply PointCloud (PLY) sparse.ply (, dense.ply) colmap_task->colmap_out_ply voxel_task Voxel romiscan.tasks.cl Computes a volume from backprojection of 2D segmented images. colmap_out_cam->voxel_task colmap_out_cam->segmentation_task mask_out Grayscale or Binary masks (PNG) mask_task->mask_out mask_out->voxel_task voxel_out Binary Voxel (NPZ) voxel_task->voxel_out pointcloud_task PointCloud romiscan.tasks.proc3d Computes a point cloud from volumetric voxel data. voxel_out->pointcloud_task pointcloud_out Plant pointcloud (PLY) pointcloud_task->pointcloud_out triangle_mesh_task TriangleMesh romiscan.tasks.proc3d Triangulates input point cloud. pointcloud_out->triangle_mesh_task triangle_mesh_out Mesh (PLY) triangle_mesh_task->triangle_mesh_out skeleton_task Skeletonization romiscan.tasks.proc3d Mesh skeletonization. triangle_mesh_out->skeleton_task skeleton_out ? (JSON) skeleton_task->skeleton_out tree_graph_task TreeGraph romiscan.tasks.arabidopsis Plant skeleton to TreeGraph structure. skeleton_out->tree_graph_task tree_graph_out TreeGraph (JSON) tree_graph_task->tree_graph_out segmentation_out Masks (PNG) segmentation_task->segmentation_out mvoxel_task Voxels romiscan.tasks.cl Computes a volume from backprojection of 2D segmented images. segmentation_out->mvoxel_task mvoxel_out Binary Voxel (NPZ) mvoxel_task->mvoxel_out lpointcloud_task PointCloud romiscan.tasks.proc3d Computes a point cloud from volumetric voxel data. mvoxel_out->lpointcloud_task lpointcloud_out Plant pointcloud (PLY) lpointcloud_task->lpointcloud_out clutered_mesh_task ClusteredMesh romiscan.tasks.proc3d Triangulates input point cloud. lpointcloud_out->clutered_mesh_task clutered_mesh_out Mesh (PLY) clutered_mesh_task->clutered_mesh_out 3D Plant Phenotyping Link dfd2 cluster_level1 Plant Phenotyping config_input Parameters (TOML) angles_and_internodes_task AnglesAndInternodes romiscan.tasks.arabidopsis Compute organs angles and internodes. config_input->angles_and_internodes_task algo_input TreeGraph (JSON) algo_input->angles_and_internodes_task ml_input ClusteredMesh (PLY) ml_input->angles_and_internodes_task angles_and_internodes_out Measures (JSON) angles_and_internodes_task->angles_and_internodes_out Evaluation Link Mask task evaluation Link dfd2 cluster_level1 Virtual plant generator cluster_level2 Algorithmic Pipeline cluster_level3 Evaluation config_input Parameters (TOML) mask_task Mask romiscan.tasks.proc2d 'Plant pixels' detection. config_input->mask_task lpy_input LPY parameters (TOML) virtualplant_task VirtualPlant romiscanner.lpy LPY 3D plant generator. lpy_input->virtualplant_task virtualplant_out1 3D Plant (OBJ) virtualplant_task->virtualplant_out1 virtualplant_out2 Angles & internodes (JSON) virtualplant_task->virtualplant_out2 virtualscan_task VirtualScan romiscanner.scan Generate photorealistic images of a virtual plant. virtualplant_out1->virtualscan_task virtualscan_out1 Multiple RGB images (PNG|JPEG) virtualscan_task->virtualscan_out1 virtualscan_out2 Camera poses (JSON) virtualscan_task->virtualscan_out2 virtualscan_out3 Ground truth segmentation (JSON) virtualscan_task->virtualscan_out3 seg2deval_task Segmentation2DEvaluation romiscan.tasks.evaluation Get ground truth voxel from virtual plant. virtualscan_out3->seg2deval_task mask_out Binary masks (PNG) mask_task->mask_out mask_out->seg2deval_task seg2deval_out Evaluate masks detection (JSON) seg2deval_task->seg2deval_out Voxel task evaluation Link dfd2 cluster_level1 Virtual plant generator cluster_level2 Algorithmic Pipeline cluster_level3 Evaluation config_input Parameters (TOML) mask_task Mask romiscan.tasks.proc2d 'Plant pixels' detection. config_input->mask_task lpy_input LPY parameters (TOML) virtualplant_task VirtualPlant romiscanner.lpy LPY 3D plant generator. lpy_input->virtualplant_task virtualplant_out1 3D Plant (OBJ) virtualplant_task->virtualplant_out1 virtualplant_out2 Angles & internodes (JSON) virtualplant_task->virtualplant_out2 voxelgroundtruth_task VoxelGroundTruth romiscan.tasks.evaluation Get ground truth voxel from virtual plant. virtualplant_out1->voxelgroundtruth_task mask_out Binary masks (PNG) mask_task->mask_out voxel_task Voxel romiscan.tasks.cl Computes a volume from backprojection of 2D segmented images. mask_out->voxel_task voxel_out Binary Voxel (NPZ) voxel_task->voxel_out voxeleval_task VoxelsEvaluation romiscan.tasks.evaluation Evaluate voxel detection based on ground truth. voxel_out->voxeleval_task voxelgroundtruth_out Binary Voxel (NPZ) voxelgroundtruth_task->voxelgroundtruth_out voxelgroundtruth_out->voxeleval_task voxeleval_out Voxel evaluation (JSON) voxeleval_task->voxeleval_out PointCloud task evaluation Link dfd2 cluster_level1 Virtual plant generator cluster_level2 Algorithmic Pipeline cluster_level3 Evaluation config_input Parameters (TOML) mask_task Mask romiscan.tasks.proc2d 'Plant pixels' detection. config_input->mask_task lpy_input LPY parameters (TOML) virtualplant_task VirtualPlant romiscanner.lpy LPY 3D plant generator. lpy_input->virtualplant_task virtualplant_out1 3D Plant (OBJ) virtualplant_task->virtualplant_out1 virtualplant_out2 Angles & internodes (JSON) virtualplant_task->virtualplant_out2 pointcloudgroundtruth_task PointCloudGroundTruth romiscan.tasks.evaluation Get ground truth point cloud from virtual plant. virtualplant_out1->pointcloudgroundtruth_task mask_out Binary masks (PNG) mask_task->mask_out voxel_task Voxel romiscan.tasks.cl Computes a volume from backprojection of 2D segmented images. mask_out->voxel_task voxel_out Binary Voxel (NPZ) voxel_task->voxel_out pointcloud_task PointCloud romiscan.tasks.proc3d Computes a point cloud from volumetric voxel data. voxel_out->pointcloud_task pointcloud_out Plant pointcloud (PLY) pointcloud_task->pointcloud_out pointcloudeval_task PointCloudEvaluation romiscan.tasks.evaluation Evaluate point cloud detection based on ground truth. pointcloud_out->pointcloudeval_task pointcloudgroundtruth_out PointCloud (PLY) pointcloudgroundtruth_task->pointcloudgroundtruth_out pointcloudgroundtruth_out->pointcloudeval_task pointcloudeval_out PointCloud evaluation (JSON) pointcloudeval_task->pointcloudeval_out","title":"Pipelines"},{"location":"Scanner/pipelines/#legend","text":"dfd2 params Input parameters task Task name module description. task_output Task output (EXT) Note shaped boxes are RomiConfig , they are TOML files that contains parameters for each task. Round shaped boxes are RomiTasks with their name on the first level, then the module names ( --module option in romi_run_task ) and a quick description of the tasks at hand. Folder shaped boxes are RomiTarget , they indicate files input/output and the file extension is given between parenthesis.","title":"Legend"},{"location":"Scanner/pipelines/#real-plant-datasets","text":"dfd2 cluster_level1 Real plants input Scanner parameters (TOML) scan_task Scan romiscanner.scan Scan a plant (requires romi scanner). input->scan_task scan_out1 Multiple RGB images (PNG|JPEG) scan_task->scan_out1 scan_out2 Approximate camera poses from CNC (JSON) scan_task->scan_out2 scan_out3 Metadata (JSON) scan_task->scan_out3","title":"Real plant datasets"},{"location":"Scanner/pipelines/#virtual-plant-datasets","text":"dfd2 cluster_level1 Virtual plant generator lpy_input LPY parameters (TOML) virtualplant_task VirtualPlant romiscanner.lpy LPY 3D plant generator. lpy_input->virtualplant_task vs_input VirtualScan parameters (TOML) virtualscan_task VirtualScan romiscanner.scan Generate photorealistic images of a virtual plant. vs_input->virtualscan_task virtualplant_out1 3D Plant (OBJ) virtualplant_task->virtualplant_out1 virtualplant_out2 Angles & internodes (JSON) virtualplant_task->virtualplant_out2 virtualplant_out1->virtualscan_task virtualscan_out1 Multiple RGB images (PNG|JPEG) virtualscan_task->virtualscan_out1 virtualscan_out2 Camera poses (JSON) virtualscan_task->virtualscan_out2","title":"Virtual plant datasets"},{"location":"Scanner/pipelines/#plant-reconstruction-from-rgb-images","text":"dfd2 cluster_level0 Algorithmic Pipeline cluster_level1 Machine Learning Pipeline input Multiple RGB images (PNG|JPEG) undistorted_task Undistorted romiscan.tasks.proc2d Undistorts images using computed intrinsic camera parameters. input->undistorted_task colmap_task Colmap romiscan.tasks.colmap Camera poses estimation. input->colmap_task mask_task Mask romiscan.tasks.proc2d 'Plant pixels' detection. input->mask_task undistorted_out Undistorted images (PNG) undistorted_task->undistorted_out undistorted_out->mask_task segmentation_task Segmentation2D romiscan.tasks.proc2d Compute masks using trained deep learning models. undistorted_out->segmentation_task colmap_out_img_md Images metadata (JSON) images.json colmap_task->colmap_out_img_md colmap_out_cam Camera poses (JSON) cameras.json colmap_task->colmap_out_cam colmap_out_pts Points in 3D (JSON) points3D.json colmap_task->colmap_out_pts colmap_out_ply PointCloud (PLY) sparse.ply (, dense.ply) colmap_task->colmap_out_ply voxel_task Voxel romiscan.tasks.cl Computes a volume from backprojection of 2D segmented images. colmap_out_cam->voxel_task colmap_out_cam->segmentation_task mask_out Grayscale or Binary masks (PNG) mask_task->mask_out mask_out->voxel_task voxel_out Binary Voxel (NPZ) voxel_task->voxel_out pointcloud_task PointCloud romiscan.tasks.proc3d Computes a point cloud from volumetric voxel data. voxel_out->pointcloud_task pointcloud_out Plant pointcloud (PLY) pointcloud_task->pointcloud_out triangle_mesh_task TriangleMesh romiscan.tasks.proc3d Triangulates input point cloud. pointcloud_out->triangle_mesh_task triangle_mesh_out Mesh (PLY) triangle_mesh_task->triangle_mesh_out skeleton_task Skeletonization romiscan.tasks.proc3d Mesh skeletonization. triangle_mesh_out->skeleton_task skeleton_out ? (JSON) skeleton_task->skeleton_out tree_graph_task TreeGraph romiscan.tasks.arabidopsis Plant skeleton to TreeGraph structure. skeleton_out->tree_graph_task tree_graph_out TreeGraph (JSON) tree_graph_task->tree_graph_out segmentation_out Masks (PNG) segmentation_task->segmentation_out mvoxel_task Voxels romiscan.tasks.cl Computes a volume from backprojection of 2D segmented images. segmentation_out->mvoxel_task mvoxel_out Binary Voxel (NPZ) mvoxel_task->mvoxel_out lpointcloud_task PointCloud romiscan.tasks.proc3d Computes a point cloud from volumetric voxel data. mvoxel_out->lpointcloud_task lpointcloud_out Plant pointcloud (PLY) lpointcloud_task->lpointcloud_out clutered_mesh_task ClusteredMesh romiscan.tasks.proc3d Triangulates input point cloud. lpointcloud_out->clutered_mesh_task clutered_mesh_out Mesh (PLY) clutered_mesh_task->clutered_mesh_out","title":"Plant Reconstruction from RGB images"},{"location":"Scanner/pipelines/#3d-plant-phenotyping","text":"dfd2 cluster_level1 Plant Phenotyping config_input Parameters (TOML) angles_and_internodes_task AnglesAndInternodes romiscan.tasks.arabidopsis Compute organs angles and internodes. config_input->angles_and_internodes_task algo_input TreeGraph (JSON) algo_input->angles_and_internodes_task ml_input ClusteredMesh (PLY) ml_input->angles_and_internodes_task angles_and_internodes_out Measures (JSON) angles_and_internodes_task->angles_and_internodes_out","title":"3D Plant Phenotyping"},{"location":"Scanner/pipelines/#evaluation","text":"","title":"Evaluation"},{"location":"Scanner/pipelines/#mask-task-evaluation","text":"dfd2 cluster_level1 Virtual plant generator cluster_level2 Algorithmic Pipeline cluster_level3 Evaluation config_input Parameters (TOML) mask_task Mask romiscan.tasks.proc2d 'Plant pixels' detection. config_input->mask_task lpy_input LPY parameters (TOML) virtualplant_task VirtualPlant romiscanner.lpy LPY 3D plant generator. lpy_input->virtualplant_task virtualplant_out1 3D Plant (OBJ) virtualplant_task->virtualplant_out1 virtualplant_out2 Angles & internodes (JSON) virtualplant_task->virtualplant_out2 virtualscan_task VirtualScan romiscanner.scan Generate photorealistic images of a virtual plant. virtualplant_out1->virtualscan_task virtualscan_out1 Multiple RGB images (PNG|JPEG) virtualscan_task->virtualscan_out1 virtualscan_out2 Camera poses (JSON) virtualscan_task->virtualscan_out2 virtualscan_out3 Ground truth segmentation (JSON) virtualscan_task->virtualscan_out3 seg2deval_task Segmentation2DEvaluation romiscan.tasks.evaluation Get ground truth voxel from virtual plant. virtualscan_out3->seg2deval_task mask_out Binary masks (PNG) mask_task->mask_out mask_out->seg2deval_task seg2deval_out Evaluate masks detection (JSON) seg2deval_task->seg2deval_out","title":"Mask task evaluation"},{"location":"Scanner/pipelines/#voxel-task-evaluation","text":"dfd2 cluster_level1 Virtual plant generator cluster_level2 Algorithmic Pipeline cluster_level3 Evaluation config_input Parameters (TOML) mask_task Mask romiscan.tasks.proc2d 'Plant pixels' detection. config_input->mask_task lpy_input LPY parameters (TOML) virtualplant_task VirtualPlant romiscanner.lpy LPY 3D plant generator. lpy_input->virtualplant_task virtualplant_out1 3D Plant (OBJ) virtualplant_task->virtualplant_out1 virtualplant_out2 Angles & internodes (JSON) virtualplant_task->virtualplant_out2 voxelgroundtruth_task VoxelGroundTruth romiscan.tasks.evaluation Get ground truth voxel from virtual plant. virtualplant_out1->voxelgroundtruth_task mask_out Binary masks (PNG) mask_task->mask_out voxel_task Voxel romiscan.tasks.cl Computes a volume from backprojection of 2D segmented images. mask_out->voxel_task voxel_out Binary Voxel (NPZ) voxel_task->voxel_out voxeleval_task VoxelsEvaluation romiscan.tasks.evaluation Evaluate voxel detection based on ground truth. voxel_out->voxeleval_task voxelgroundtruth_out Binary Voxel (NPZ) voxelgroundtruth_task->voxelgroundtruth_out voxelgroundtruth_out->voxeleval_task voxeleval_out Voxel evaluation (JSON) voxeleval_task->voxeleval_out","title":"Voxel task evaluation"},{"location":"Scanner/pipelines/#pointcloud-task-evaluation","text":"dfd2 cluster_level1 Virtual plant generator cluster_level2 Algorithmic Pipeline cluster_level3 Evaluation config_input Parameters (TOML) mask_task Mask romiscan.tasks.proc2d 'Plant pixels' detection. config_input->mask_task lpy_input LPY parameters (TOML) virtualplant_task VirtualPlant romiscanner.lpy LPY 3D plant generator. lpy_input->virtualplant_task virtualplant_out1 3D Plant (OBJ) virtualplant_task->virtualplant_out1 virtualplant_out2 Angles & internodes (JSON) virtualplant_task->virtualplant_out2 pointcloudgroundtruth_task PointCloudGroundTruth romiscan.tasks.evaluation Get ground truth point cloud from virtual plant. virtualplant_out1->pointcloudgroundtruth_task mask_out Binary masks (PNG) mask_task->mask_out voxel_task Voxel romiscan.tasks.cl Computes a volume from backprojection of 2D segmented images. mask_out->voxel_task voxel_out Binary Voxel (NPZ) voxel_task->voxel_out pointcloud_task PointCloud romiscan.tasks.proc3d Computes a point cloud from volumetric voxel data. voxel_out->pointcloud_task pointcloud_out Plant pointcloud (PLY) pointcloud_task->pointcloud_out pointcloudeval_task PointCloudEvaluation romiscan.tasks.evaluation Evaluate point cloud detection based on ground truth. pointcloud_out->pointcloudeval_task pointcloudgroundtruth_out PointCloud (PLY) pointcloudgroundtruth_task->pointcloudgroundtruth_out pointcloudgroundtruth_out->pointcloudeval_task pointcloudeval_out PointCloud evaluation (JSON) pointcloudeval_task->pointcloudeval_out","title":"PointCloud task evaluation"},{"location":"Scanner/Developer/conda/","text":"Conda Link Recipes to build conda packages can be found here . Follow theses instruction to build conda packages. Warning Conda packages should be built from the base environment. conda activate base Requirements Link Install conda-build : Link Install conda-build , in the base environment, to be able to build conda package: conda install conda-build WARNING: For macOS, follow these instructions to install the required macOS 10.9 SDK . Optional - Install anaconda-client : Link To be able to upload your package on anaconda cloud you need to install anaconda-client : conda install anaconda-client Build conda packages: Link Build lettucethink : Link Using the given recipe, it is easy to build the lettucethink-python conda package: cd conda_recipes/ conda build lettucethink/ --user romi-eu Build romidata : Link Using the given recipe, it is easy to build the romidata conda package: cd conda_recipes/ conda build romidata/ -c romi-eu -c open3d-admin --user romi-eu Build romiscan : Link Using the given recipe, it is easy to build the romiscan conda package: cd conda_recipes/ conda build romiscan/ -c romi-eu -c conda-forge -c open3d-admin --user romi-eu Build romi-plantviz : Link Using the given recipe, it is easy to build the romi-plantviz conda package: cd conda_recipes/ conda build romi-plantviz/ -c romi-eu -c conda-forge --user romi-eu Optional - Build dirsync package: Link To build dirsync you have to install hgsvn : sudo apt install hgsvn Using the given recipe, it is easy to build the dirsync conda package: cd conda_recipes conda build dirsync/recipe/ --user romi-eu Optional - Build opencv-python package: Link To build opencv-python you have to install qt4-qmake : sudo apt install qt4-qmake qt4-default Using the given recipe, it is easy to build the opencv-python conda package: cd conda_recipes conda build opencv-python/ -c conda-forge --user romi-eu Conda useful commands: Link Purge built packages: Link conda build purge Clean cache & unused packages: Link conda clean --all","title":"Conda"},{"location":"Scanner/Developer/conda/#conda","text":"Recipes to build conda packages can be found here . Follow theses instruction to build conda packages. Warning Conda packages should be built from the base environment. conda activate base","title":"Conda"},{"location":"Scanner/Developer/conda/#requirements","text":"","title":"Requirements"},{"location":"Scanner/Developer/conda/#install-conda-build","text":"Install conda-build , in the base environment, to be able to build conda package: conda install conda-build WARNING: For macOS, follow these instructions to install the required macOS 10.9 SDK .","title":"Install conda-build:"},{"location":"Scanner/Developer/conda/#optional-install-anaconda-client","text":"To be able to upload your package on anaconda cloud you need to install anaconda-client : conda install anaconda-client","title":"Optional - Install anaconda-client:"},{"location":"Scanner/Developer/conda/#build-conda-packages","text":"","title":"Build conda packages:"},{"location":"Scanner/Developer/conda/#build-lettucethink","text":"Using the given recipe, it is easy to build the lettucethink-python conda package: cd conda_recipes/ conda build lettucethink/ --user romi-eu","title":"Build lettucethink:"},{"location":"Scanner/Developer/conda/#build-romidata","text":"Using the given recipe, it is easy to build the romidata conda package: cd conda_recipes/ conda build romidata/ -c romi-eu -c open3d-admin --user romi-eu","title":"Build romidata:"},{"location":"Scanner/Developer/conda/#build-romiscan","text":"Using the given recipe, it is easy to build the romiscan conda package: cd conda_recipes/ conda build romiscan/ -c romi-eu -c conda-forge -c open3d-admin --user romi-eu","title":"Build romiscan:"},{"location":"Scanner/Developer/conda/#build-romi-plantviz","text":"Using the given recipe, it is easy to build the romi-plantviz conda package: cd conda_recipes/ conda build romi-plantviz/ -c romi-eu -c conda-forge --user romi-eu","title":"Build romi-plantviz:"},{"location":"Scanner/Developer/conda/#optional-build-dirsync-package","text":"To build dirsync you have to install hgsvn : sudo apt install hgsvn Using the given recipe, it is easy to build the dirsync conda package: cd conda_recipes conda build dirsync/recipe/ --user romi-eu","title":"Optional - Build dirsync package:"},{"location":"Scanner/Developer/conda/#optional-build-opencv-python-package","text":"To build opencv-python you have to install qt4-qmake : sudo apt install qt4-qmake qt4-default Using the given recipe, it is easy to build the opencv-python conda package: cd conda_recipes conda build opencv-python/ -c conda-forge --user romi-eu","title":"Optional - Build opencv-python package:"},{"location":"Scanner/Developer/conda/#conda-useful-commands","text":"","title":"Conda useful commands:"},{"location":"Scanner/Developer/conda/#purge-built-packages","text":"conda build purge","title":"Purge built packages:"},{"location":"Scanner/Developer/conda/#clean-cache-unused-packages","text":"conda clean --all","title":"Clean cache &amp; unused packages:"},{"location":"Scanner/Developer/docker/","text":"Docker containers for ROMI Link List of docker containers Link We hereafter list the docker containers, their use and list the installed libraries: Progress: - [ ] DB - [ ] Scanner - [ ] SmartInterpreter - [ ] VirtualScanner - [x] Visualizer romidb Link FROM ubuntu:18.04 Warning This is not working YET, but almost! Build docker image Link To build the image, clone the ROMI docker repository: git clone https://github.com/romi/docker.git cd docker Then you can build the image with: docker docker build -t romidb romiDB/ To run it, you need to have a local database first, look here for an example. Assuming you extracted it in your home folder ( /home/$USER/integration_tests ), you can start the romidb docker image with: docker run -it -p 5000 :5000 -v /home/ $USER /integration_tests:/home/romi/integration_tests romidb Once it's up, you should be able to access the REST API here: http://localhost:5000/ For example to get a list of the scans: http://localhost:5000/scans Note -v /home/$USER/integration_tests:/home/romi/integration_tests performs a bind mount to enable access to the local database by the docker image. See the official documentation . Push it ot docker hub: docker push jlegrand62/romi_database:latest This require a valid account, token and existing repository ( romi_database ) on docker hub! Use pre-built docker image Link First you need to pull the docker image: docker pull jlegrand62/romi_database Then you can run it with: docker run -it -p 3000 :3000 jlegrand62/romi_database Note ROMI does not have a docker repo yet! Scanner Link The database container. Aim Link Gather the acquired images, reconstructed plants architectures & quantitative data FROM continuumio/miniconda3 Dependencies: Link Internal dependencies: - data-storage , the database API, as well as classes for data processing using luigi. External dependencies: - lettucethink-python , python tools and controllers for the lettucethink robot. SmartInterpreter Link The reconstruction and quantification container. Aim Link Gather the algorithms and processing pipelines to extract quantitative biological information on plants architecture from a set of photo acquisition. FROM colmap/colmap Dependencies: Link Internal dependencies: - data-storage , the database API, as well as classes for data processing using luigi. - Segmentation , virtual plant segmentation methods using 2D images generated from the virtual scanner and neural networks. - cgal_bindings_skeletonization , Python CGAL bindings for skeletonization. - Scan3D , the elements used to run 3D scan of individual plants. External dependencies: - Open3D is mostly used for IO and data structures. - Colmap is used for Structure From Motion reconstruction. - cgal is used for skeletonization. TO REMOVE ?! VirtualScanner Link The virtual scanner container. Aim Link Gather the tools to create virtual plants and render realistic snapshots. FROM alpine:3.10 Dependencies: Link Internal dependencies: - data-storage , the database API, as well as classes for data processing using luigi. External dependencies: - blender - lpy plantviewer Link The plant visualizer is a webapp that dialog with the database to display images & some quantitative traits. FROM ubuntu:18.04 Build docker image Link To build the image, clone the ROMI docker repository: git clone https://github.com/romi/docker.git cd docker Then you can build the image with: docker build -t visualizer Visualizer/ To run it: docker run -it -p 3000 :3000 visualizer Once it's up, you should be able to access the viewer here: http://localhost:3000/ Important Use chrome as firefox has some issues with the used JavaScript libraries! Push it ot docker hub: docker push jlegrand62/romi_plantviewer:latest This require a valid account, token and existing repository ( romi_plantviewer ) on docker hub! Use pre-built docker image Link First you need to pull the docker image: docker pull jlegrand62/romi_plantviewer Then you can run it with: docker run -it -p 3000 :3000 jlegrand62/romi_plantviewer Note ROMI does not have a docker repo yet! DockerHub Link Colmap Link Docker images for the COLMAP open source project: https://hub.docker.com/r/colmap/colmap nvidia/cuda with colmap - (compatible with Driver Version: 418.67 CUDA Version: 10.1) https://hub.docker.com/r/geki/colmap","title":"Docker"},{"location":"Scanner/Developer/docker/#docker-containers-for-romi","text":"","title":"Docker containers for ROMI"},{"location":"Scanner/Developer/docker/#list-of-docker-containers","text":"We hereafter list the docker containers, their use and list the installed libraries: Progress: - [ ] DB - [ ] Scanner - [ ] SmartInterpreter - [ ] VirtualScanner - [x] Visualizer","title":"List of docker containers"},{"location":"Scanner/Developer/docker/#romidb","text":"FROM ubuntu:18.04 Warning This is not working YET, but almost!","title":"romidb"},{"location":"Scanner/Developer/docker/#build-docker-image","text":"To build the image, clone the ROMI docker repository: git clone https://github.com/romi/docker.git cd docker Then you can build the image with: docker docker build -t romidb romiDB/ To run it, you need to have a local database first, look here for an example. Assuming you extracted it in your home folder ( /home/$USER/integration_tests ), you can start the romidb docker image with: docker run -it -p 5000 :5000 -v /home/ $USER /integration_tests:/home/romi/integration_tests romidb Once it's up, you should be able to access the REST API here: http://localhost:5000/ For example to get a list of the scans: http://localhost:5000/scans Note -v /home/$USER/integration_tests:/home/romi/integration_tests performs a bind mount to enable access to the local database by the docker image. See the official documentation . Push it ot docker hub: docker push jlegrand62/romi_database:latest This require a valid account, token and existing repository ( romi_database ) on docker hub!","title":"Build docker image"},{"location":"Scanner/Developer/docker/#use-pre-built-docker-image","text":"First you need to pull the docker image: docker pull jlegrand62/romi_database Then you can run it with: docker run -it -p 3000 :3000 jlegrand62/romi_database Note ROMI does not have a docker repo yet!","title":"Use pre-built docker image"},{"location":"Scanner/Developer/docker/#scanner","text":"The database container.","title":"Scanner"},{"location":"Scanner/Developer/docker/#aim","text":"Gather the acquired images, reconstructed plants architectures & quantitative data FROM continuumio/miniconda3","title":"Aim"},{"location":"Scanner/Developer/docker/#dependencies","text":"Internal dependencies: - data-storage , the database API, as well as classes for data processing using luigi. External dependencies: - lettucethink-python , python tools and controllers for the lettucethink robot.","title":"Dependencies:"},{"location":"Scanner/Developer/docker/#smartinterpreter","text":"The reconstruction and quantification container.","title":"SmartInterpreter"},{"location":"Scanner/Developer/docker/#aim_1","text":"Gather the algorithms and processing pipelines to extract quantitative biological information on plants architecture from a set of photo acquisition. FROM colmap/colmap","title":"Aim"},{"location":"Scanner/Developer/docker/#dependencies_1","text":"Internal dependencies: - data-storage , the database API, as well as classes for data processing using luigi. - Segmentation , virtual plant segmentation methods using 2D images generated from the virtual scanner and neural networks. - cgal_bindings_skeletonization , Python CGAL bindings for skeletonization. - Scan3D , the elements used to run 3D scan of individual plants. External dependencies: - Open3D is mostly used for IO and data structures. - Colmap is used for Structure From Motion reconstruction. - cgal is used for skeletonization. TO REMOVE ?!","title":"Dependencies:"},{"location":"Scanner/Developer/docker/#virtualscanner","text":"The virtual scanner container.","title":"VirtualScanner"},{"location":"Scanner/Developer/docker/#aim_2","text":"Gather the tools to create virtual plants and render realistic snapshots. FROM alpine:3.10","title":"Aim"},{"location":"Scanner/Developer/docker/#dependencies_2","text":"Internal dependencies: - data-storage , the database API, as well as classes for data processing using luigi. External dependencies: - blender - lpy","title":"Dependencies:"},{"location":"Scanner/Developer/docker/#plantviewer","text":"The plant visualizer is a webapp that dialog with the database to display images & some quantitative traits. FROM ubuntu:18.04","title":"plantviewer"},{"location":"Scanner/Developer/docker/#build-docker-image_1","text":"To build the image, clone the ROMI docker repository: git clone https://github.com/romi/docker.git cd docker Then you can build the image with: docker build -t visualizer Visualizer/ To run it: docker run -it -p 3000 :3000 visualizer Once it's up, you should be able to access the viewer here: http://localhost:3000/ Important Use chrome as firefox has some issues with the used JavaScript libraries! Push it ot docker hub: docker push jlegrand62/romi_plantviewer:latest This require a valid account, token and existing repository ( romi_plantviewer ) on docker hub!","title":"Build docker image"},{"location":"Scanner/Developer/docker/#use-pre-built-docker-image_1","text":"First you need to pull the docker image: docker pull jlegrand62/romi_plantviewer Then you can run it with: docker run -it -p 3000 :3000 jlegrand62/romi_plantviewer Note ROMI does not have a docker repo yet!","title":"Use pre-built docker image"},{"location":"Scanner/Developer/docker/#dockerhub","text":"","title":"DockerHub"},{"location":"Scanner/Developer/docker/#colmap","text":"Docker images for the COLMAP open source project: https://hub.docker.com/r/colmap/colmap nvidia/cuda with colmap - (compatible with Driver Version: 418.67 CUDA Version: 10.1) https://hub.docker.com/r/geki/colmap","title":"Colmap"},{"location":"Scanner/How-To/basics/","text":"How to use the ROMI scanner software? Link This is a document centralizing all documentation for the 3D scanner. The 3D scanner software is composed of several python libraries organized in different packages: romidata : the data processing module lettucethink : the hardware interface romiscanner : the scanner interface and the virtual scanner romiscan : the computer vision algorithms romiseg : the segmentation models Additionally, some CGAL bindings are implemented in a separate python library: cgal_bindings_skeletonization . A separate repository is dedicated to the virtual scanner, which is available as a blender python (bpy) script. Getting started Link There are some requirements to use the different algorithms in the pipeline. Most of them are installed automatically from the requirements file when using pip. The most important part is Colmap (v3.6). The two requirements that are not shipped with pip are: Colmap (v3.6) for the structure from motion algorithms Blender (>= 2.81) for the virtual scanner Preferably, create a virtual environment for python 3.7 or python 3.8 using virtualenv or a conda environment specific to the 3D Scanner. Warning If using python 3.8, Open3D binaries are not yet available on pip, therefore you have to build Open3D from sources! How to - Install ROMI packages Link Choose between a Python venv or a conda environment (A or B). A - Create a virtual environment Link To create a venv named scan3d with Python 3.7: virtualenv -p /usr/bin/python3.7 scan3d Note This should probably be replaced by: python3.7 -m venv scan3d Then activate it with: source scan3d/activate Now you can now easily install Python packages, for example NumPy , as follow: pip install numpy B - Create a conda environment: Link To create a conda environment named scan3d with Python 3.7: conda create -n scan3d python == 3 .7 Then activate it: conda activate scan3d Now you can now easily install Python packages, for example NumPy , as follow: conda install numpy How to - Initialize a database Link The FSDB class from the romidata module is used for data storage. A database is any folder which contains a file named romidb . To create an empty database, just create a new folder and an empty file named romidb in it. For example: mkdir /path/to/my/db cd /path/to/my/db touch romidb Basic usage Link Every task on the scanner is launched through the romi_run_task command provided in the romiscan module. It is a wrapper for luigi , with preloaded tasks from the romiscan module. The general usage is as follows: romi_run_task [ -h ] [ --config CONFIG ] [ --luigicmd LUIGICMD ] [ --module MODULE ] [ --local-scheduler ] [ --log-level LOG_LEVEL ] task scan CONFIG is either a file or a folder. If a file, it must be json or toml and contains the configuration of the task to run. If a folder, it will read all configuration files in json or toml format from the folder. LUIGICMD is an optional parameter specifying an alternative command for luigi . MODULE is an optional parameter for running task from external modules (see TODO). LOG_LEVEL is the level of logging. Defaults to INFO , but can be set to DEBUG to increase verbosity. task is the name of the class to run (see TODO) scan is the location of the target scan on which to process the task. It is of the form DB_LOCATION/SCAN_ID , where DB_LOCATION is a path containing the romidb marker. Configuration files Link The configuration is in the form of a dictionary, in which each key is the ID of a given task. In toml format, it reads as follows: [FirstTask] parameter1 = value1 parameter2 = value2 [SecondTask] parameter1 = value1 parameter2 = value2 Pipelines Link This is a sample configuration for the full reconstruction pipeline : [Colmap] matcher = \"exhaustive\" compute_dense = false [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" [Masks] type = \"excess_green\" dilation = 5 binarize = true threshold = 0.0 [Voxels] voxel_size = 1.0 type = \"carving\" [PointCloud] level_set_value = 1.0 [Visualization] max_image_size = 1500 max_pcd_size = 10000 thumbnail_size = 150 pcd_source = \"vox2pcd\" mesh_source = \"delaunay\" To run the full reconstruction pipeline use this configuration file with romi_run_task : romi_run_task --config scanner.json AnglesAndInternodes /path/to/db/scan_id/ --local-scheduler This will process all tasks up to the AnglesAndInternodes task. Every task produces a Fileset , a subdirectory in the scan directory whose name starts the same as the task name. The characters following are a hash of the configuration of the task, so that the outputs of the same task with different parameters can coexist in the same scan. Any change in the parameters will make the needed task to be recomputed with subsequent calls of romi_run_task . Already computed tasks will be left untouched. To recompute a task, just delete the corresponding folder in the scan directory and rerun romi_run_task . Default task reference Link default_modules = { \"Scan\": \"romiscan.tasks.scan\", \"Clean\": \"romiscan.tasks.scan\", \"CalibrationScan\": \"romiscan.tasks.scan\", \"Colmap\": \"romiscan.tasks.colmap\", \"Undistorted\": \"romiscan.tasks.proc2d\", \"Masks\": \"romiscan.tasks.proc2d\", \"Segmentation2D\": \"romiscan.tasks.proc2d\", \"Voxels\": \"romiscan.tasks.cl\", \"PointCloud\": \"romiscan.tasks.proc3d\", \"TriangleMesh\": \"romiscan.tasks.proc3d\", \"CurveSkeleton\": \"romiscan.tasks.proc3d\", \"TreeGraph\": \"romiscan.tasks.arabidopsis\", \"AnglesAndInternodes\": \"romiscan.tasks.arabidopsis\", \"Visualization\": \"romiscan.tasks.visualization\" } Warning This is for reference only, please update the changes in the code.** This will be later replaced by a reference doc generated from the code! Class name : Scan Module : romiscan . tasks . scan Description : A task for running a scan , real or virtual . Default upstream tasks : None Parameters : - metadata ( DictParameter ) : metadata for the scan - scanner ( DictParameter ) : scanner hardware configuration ( TODO : see hardware documentation ) - path ( DictParameter ) : scanner path configuration ( TODO : see hardware documentation ) Class name : CalibrationScan Module : romiscan . tasks . scan Description : A task for running a scan , real or virtual , with a calibration path . It is used to calibrate Colmap poses for subsequent scans . ( TODO : see calibration documentation ) Default upstream tasks : None Parameters : - metadata ( DictParameter ) : metadata for the scan - scanner ( DictParameter ) : scanner hardware configuration ( TODO : see hardware documentation ) - path ( DictParameter ) : scanner path configuration ( TODO : see hardware documentation ) - n_line : number of shots taken on the orthogonal calibration lines Class name : Clean Module : romiscan . tasks . scan Description : Cleanup a scan , keeping only the \"images\" fileset and removing all computed pipelines . Default upstream tasks : None Parameters : - no_confirm ( BoolParameter , default = False ) : do not ask for confirmation in the command prompt . Class name : Colmap Module : romiscan . tasks . colmap Description : Runs colmap on a given scan . Default upstream tasks : Scan Upstream task format : Fileset with image files Output fileset format : images . json , cameras . json , points3D . json , sparse . ply [ , dense . ply ] Parameters : - matcher ( Parameter , default = \"exhaustive\" ) : either \"exhaustive\" or \"sequential\" ( TODO : see colmap documentation ) - compute_dense ( BoolParameter ) : whether to run the dense colmap to obtain a dense point cloud - cli_args ( DictParameter ) : parameters for colmap command line prompts ( TODO : see colmap documentation ) - align_pcd ( BoolParameter , default = True ) : align point cloud on calibrated or metadata poses ? - calibration_scan_id ( Parameter , default = \"\" ) : ID of the calibration scan . Class name : Undistorted Module : romiscan . tasks . proc2d Description : Undistorts images using computed intrinsic camera parameters Default upstream tasks : Scan , Colmap Upstream task format : Fileset with image files Output fileset format : Fileset with image files Class name : Masks Module : romiscan . tasks . proc2d Description : compute masks using several functions Default upstream tasks : Undistorted Upstream task format : Fileset with image files Output fileset format : Fileset with grayscale or binary image files Parameters : - type ( Parameter ) : \"linear\" , \"excess_green\" , \"vesselness\" , \"invert\" ( TODO : see segmentation documentation ) - parameters ( ListParameter ) : list of scalar parmeters , depends on type - dilation ( IntParameter ) : by how much to dilate masks if binary - binarize ( BoolParameter , default = True ) : binarize the masks - threshold ( FloatParameter , default = 0.0 ) : threshold for binarization - Class name : Segmentation2D Module : romiscan . tasks . proc2d Description : compute masks using trained deep learning models Default upstream tasks : Undistorted Upstream task format : Fileset with image files Output fileset format : Fileset with grayscale image files , each corresponding to a given input image and class Parameters : - query ( DictParameter ) : query to pass to upstream fileset . It filters file by metadata , e . g { \"channel\" : \"rgb\" } will process only input files such that \"channel\" metadata is equal to \"rgb\" . - labels ( Parameter ) : string of the form \"a,b,c\" such that a , b , c are the identifiers of the labels produced by the neural network - Sx , Sy ( IntParametr ) : size of the input of the neural network . Input pictures are cropped in the center to this size . - model_segmentation_name : name of \".pt\" file that can be found at ` https : // db . romi - project . eu / models ` - Class name : Voxels Module : romiscan . tasks . cl Description : Computes a volume from backprojection of 2D segmented images Default upstream tasks : - upstream_mask : Masks - upstream_colmap : Colmap Upstream task format : - upstream_mask : Fileset with grayscale images - upstream_colmap : Output of Colmap task Output fileset format : npz file with as many arrays as classes Parameters : - use_colmap_poses ( BoolParameter , default = True ): Either use precomputed camera poses or output from the Colmap task - voxel_size ( FloatParameter ): size of one side of voxels - type ( Parameter ): \"carving\" or \"averaging\" ( TODO : See 3D documentation ) - multiclass ( BoolParameter , default = False ): whether input data is single class or multiclass ( e . g as an output of Segmentation2D ) - log ( BoolParameter , default = True ), in the case of \"averaging\" type , whether to apply log when averaging values . Class name : PointCloud Module : romiscan . tasks . proc3d Description : Computes a point cloud from volumetric voxel data ( either single or multiclass ) Default upstream tasks : Voxels Upstream task format : npz file with as many 3D array as classes Output task format : single point cloud in ply . Metadata may include label name if multiclass . Class name : TriangleMesh Module : romiscan . tasks . proc3d Description : Triangulates input point cloud . Currently ignores class data and needs only one connected component . Default upstream tasks : PointCloud Upstream task format : ply file Output task format : ply triangle mesh file Class name : CurveSkeleton Module : romiscan . tasks . proc3d Description : Creates a 3D curve skeleton Default upstream tasks : TriangleMesh Upstream task format : ply triangle mesh Output task format : json with two entries \"points\" and \"lines\" ( TODO : precise ) Class name : TreeGraph Module : romiscan . tasks . arabidopsis Description : Creates a tree graph of the plant Default upstream tasks : CurveSkeleton Upstream task format : json Output task format : json ( TODO : precise ) Class name ; AnglesAndInternodes Module : romiscan . tasks . arabidopsis Description : Computes angles and internode Default upstream tasks : TreeGraph Upstream task format : json Output task format : json ( TODO : precise ) Scanner API reference Link Objects Link /objects (GET): retrieve the list of obj files in the data folder that can be loaded. /load_object/<object_id> (GET) load the given object in the scene. Takes a translation vector as URL parameters ( dx , dy , dz ) Classes Link /classes (GET): retrieve the list of classes. Backgrounds Link /backgrounds (GET): retrieve the list of hdr files in the hdri folder that can be loaded. /load_background/<background_id> (GET) load the given background in the scene. Camera Link /camera_intrinsics (POST): set camera intrinsics. Keys: width , height , focal /camera_pose (POST): set camera pose. Keys: tx , ty , tz , rx , ry , rz Rendering Link /render (GET): gets the rendering of the scene /render_class/<class_id> (GET) renders the scene, with everything transparent except the given class TODO: missing endpoints httpie # Setup camera http - f post http : // localhost : 5000 / camera_intrinsics width = 1920 height = 1080 focal = 35 # Load arabidopsis_0 http get 'http://localhost:5000/load_object/arabidopsis_0.obj?dx=10&dy=20&dz=1' # Load \"old tree in the park\" background http get http : // 127 . 0 . 0 . 1 : 5000 / load_background / old_tree_in_city_park_8k . hdr # Move camera http - f post http : // localhost : 5000 / camera_pose tx =- 60 ty = 0 tz = 50 rx = 60 ry = 0 rz =- 90 # Render scene and download image http --download get http://localhost:5000/render # Render only leaves http --download get http://localhost:5000/render_class/Color_7","title":"Basics"},{"location":"Scanner/How-To/basics/#how-to-use-the-romi-scanner-software","text":"This is a document centralizing all documentation for the 3D scanner. The 3D scanner software is composed of several python libraries organized in different packages: romidata : the data processing module lettucethink : the hardware interface romiscanner : the scanner interface and the virtual scanner romiscan : the computer vision algorithms romiseg : the segmentation models Additionally, some CGAL bindings are implemented in a separate python library: cgal_bindings_skeletonization . A separate repository is dedicated to the virtual scanner, which is available as a blender python (bpy) script.","title":"How to use the ROMI scanner software?"},{"location":"Scanner/How-To/basics/#getting-started","text":"There are some requirements to use the different algorithms in the pipeline. Most of them are installed automatically from the requirements file when using pip. The most important part is Colmap (v3.6). The two requirements that are not shipped with pip are: Colmap (v3.6) for the structure from motion algorithms Blender (>= 2.81) for the virtual scanner Preferably, create a virtual environment for python 3.7 or python 3.8 using virtualenv or a conda environment specific to the 3D Scanner. Warning If using python 3.8, Open3D binaries are not yet available on pip, therefore you have to build Open3D from sources!","title":"Getting started"},{"location":"Scanner/How-To/basics/#how-to-install-romi-packages","text":"Choose between a Python venv or a conda environment (A or B).","title":"How to - Install ROMI packages"},{"location":"Scanner/How-To/basics/#a-create-a-virtual-environment","text":"To create a venv named scan3d with Python 3.7: virtualenv -p /usr/bin/python3.7 scan3d Note This should probably be replaced by: python3.7 -m venv scan3d Then activate it with: source scan3d/activate Now you can now easily install Python packages, for example NumPy , as follow: pip install numpy","title":"A - Create a virtual environment"},{"location":"Scanner/How-To/basics/#b-create-a-conda-environment","text":"To create a conda environment named scan3d with Python 3.7: conda create -n scan3d python == 3 .7 Then activate it: conda activate scan3d Now you can now easily install Python packages, for example NumPy , as follow: conda install numpy","title":"B - Create a conda environment:"},{"location":"Scanner/How-To/basics/#how-to-initialize-a-database","text":"The FSDB class from the romidata module is used for data storage. A database is any folder which contains a file named romidb . To create an empty database, just create a new folder and an empty file named romidb in it. For example: mkdir /path/to/my/db cd /path/to/my/db touch romidb","title":"How to - Initialize a database"},{"location":"Scanner/How-To/basics/#basic-usage","text":"Every task on the scanner is launched through the romi_run_task command provided in the romiscan module. It is a wrapper for luigi , with preloaded tasks from the romiscan module. The general usage is as follows: romi_run_task [ -h ] [ --config CONFIG ] [ --luigicmd LUIGICMD ] [ --module MODULE ] [ --local-scheduler ] [ --log-level LOG_LEVEL ] task scan CONFIG is either a file or a folder. If a file, it must be json or toml and contains the configuration of the task to run. If a folder, it will read all configuration files in json or toml format from the folder. LUIGICMD is an optional parameter specifying an alternative command for luigi . MODULE is an optional parameter for running task from external modules (see TODO). LOG_LEVEL is the level of logging. Defaults to INFO , but can be set to DEBUG to increase verbosity. task is the name of the class to run (see TODO) scan is the location of the target scan on which to process the task. It is of the form DB_LOCATION/SCAN_ID , where DB_LOCATION is a path containing the romidb marker.","title":"Basic usage"},{"location":"Scanner/How-To/basics/#configuration-files","text":"The configuration is in the form of a dictionary, in which each key is the ID of a given task. In toml format, it reads as follows: [FirstTask] parameter1 = value1 parameter2 = value2 [SecondTask] parameter1 = value1 parameter2 = value2","title":"Configuration files"},{"location":"Scanner/How-To/basics/#pipelines","text":"This is a sample configuration for the full reconstruction pipeline : [Colmap] matcher = \"exhaustive\" compute_dense = false [Colmap.cli_args.feature_extractor] \"--ImageReader.single_camera\" = \"1\" \"--SiftExtraction.use_gpu\" = \"1\" [Colmap.cli_args.exhaustive_matcher] \"--SiftMatching.use_gpu\" = \"1\" [Colmap.cli_args.model_aligner] \"--robust_alignment_max_error\" = \"10\" [Masks] type = \"excess_green\" dilation = 5 binarize = true threshold = 0.0 [Voxels] voxel_size = 1.0 type = \"carving\" [PointCloud] level_set_value = 1.0 [Visualization] max_image_size = 1500 max_pcd_size = 10000 thumbnail_size = 150 pcd_source = \"vox2pcd\" mesh_source = \"delaunay\" To run the full reconstruction pipeline use this configuration file with romi_run_task : romi_run_task --config scanner.json AnglesAndInternodes /path/to/db/scan_id/ --local-scheduler This will process all tasks up to the AnglesAndInternodes task. Every task produces a Fileset , a subdirectory in the scan directory whose name starts the same as the task name. The characters following are a hash of the configuration of the task, so that the outputs of the same task with different parameters can coexist in the same scan. Any change in the parameters will make the needed task to be recomputed with subsequent calls of romi_run_task . Already computed tasks will be left untouched. To recompute a task, just delete the corresponding folder in the scan directory and rerun romi_run_task .","title":"Pipelines"},{"location":"Scanner/How-To/basics/#default-task-reference","text":"default_modules = { \"Scan\": \"romiscan.tasks.scan\", \"Clean\": \"romiscan.tasks.scan\", \"CalibrationScan\": \"romiscan.tasks.scan\", \"Colmap\": \"romiscan.tasks.colmap\", \"Undistorted\": \"romiscan.tasks.proc2d\", \"Masks\": \"romiscan.tasks.proc2d\", \"Segmentation2D\": \"romiscan.tasks.proc2d\", \"Voxels\": \"romiscan.tasks.cl\", \"PointCloud\": \"romiscan.tasks.proc3d\", \"TriangleMesh\": \"romiscan.tasks.proc3d\", \"CurveSkeleton\": \"romiscan.tasks.proc3d\", \"TreeGraph\": \"romiscan.tasks.arabidopsis\", \"AnglesAndInternodes\": \"romiscan.tasks.arabidopsis\", \"Visualization\": \"romiscan.tasks.visualization\" } Warning This is for reference only, please update the changes in the code.** This will be later replaced by a reference doc generated from the code! Class name : Scan Module : romiscan . tasks . scan Description : A task for running a scan , real or virtual . Default upstream tasks : None Parameters : - metadata ( DictParameter ) : metadata for the scan - scanner ( DictParameter ) : scanner hardware configuration ( TODO : see hardware documentation ) - path ( DictParameter ) : scanner path configuration ( TODO : see hardware documentation ) Class name : CalibrationScan Module : romiscan . tasks . scan Description : A task for running a scan , real or virtual , with a calibration path . It is used to calibrate Colmap poses for subsequent scans . ( TODO : see calibration documentation ) Default upstream tasks : None Parameters : - metadata ( DictParameter ) : metadata for the scan - scanner ( DictParameter ) : scanner hardware configuration ( TODO : see hardware documentation ) - path ( DictParameter ) : scanner path configuration ( TODO : see hardware documentation ) - n_line : number of shots taken on the orthogonal calibration lines Class name : Clean Module : romiscan . tasks . scan Description : Cleanup a scan , keeping only the \"images\" fileset and removing all computed pipelines . Default upstream tasks : None Parameters : - no_confirm ( BoolParameter , default = False ) : do not ask for confirmation in the command prompt . Class name : Colmap Module : romiscan . tasks . colmap Description : Runs colmap on a given scan . Default upstream tasks : Scan Upstream task format : Fileset with image files Output fileset format : images . json , cameras . json , points3D . json , sparse . ply [ , dense . ply ] Parameters : - matcher ( Parameter , default = \"exhaustive\" ) : either \"exhaustive\" or \"sequential\" ( TODO : see colmap documentation ) - compute_dense ( BoolParameter ) : whether to run the dense colmap to obtain a dense point cloud - cli_args ( DictParameter ) : parameters for colmap command line prompts ( TODO : see colmap documentation ) - align_pcd ( BoolParameter , default = True ) : align point cloud on calibrated or metadata poses ? - calibration_scan_id ( Parameter , default = \"\" ) : ID of the calibration scan . Class name : Undistorted Module : romiscan . tasks . proc2d Description : Undistorts images using computed intrinsic camera parameters Default upstream tasks : Scan , Colmap Upstream task format : Fileset with image files Output fileset format : Fileset with image files Class name : Masks Module : romiscan . tasks . proc2d Description : compute masks using several functions Default upstream tasks : Undistorted Upstream task format : Fileset with image files Output fileset format : Fileset with grayscale or binary image files Parameters : - type ( Parameter ) : \"linear\" , \"excess_green\" , \"vesselness\" , \"invert\" ( TODO : see segmentation documentation ) - parameters ( ListParameter ) : list of scalar parmeters , depends on type - dilation ( IntParameter ) : by how much to dilate masks if binary - binarize ( BoolParameter , default = True ) : binarize the masks - threshold ( FloatParameter , default = 0.0 ) : threshold for binarization - Class name : Segmentation2D Module : romiscan . tasks . proc2d Description : compute masks using trained deep learning models Default upstream tasks : Undistorted Upstream task format : Fileset with image files Output fileset format : Fileset with grayscale image files , each corresponding to a given input image and class Parameters : - query ( DictParameter ) : query to pass to upstream fileset . It filters file by metadata , e . g { \"channel\" : \"rgb\" } will process only input files such that \"channel\" metadata is equal to \"rgb\" . - labels ( Parameter ) : string of the form \"a,b,c\" such that a , b , c are the identifiers of the labels produced by the neural network - Sx , Sy ( IntParametr ) : size of the input of the neural network . Input pictures are cropped in the center to this size . - model_segmentation_name : name of \".pt\" file that can be found at ` https : // db . romi - project . eu / models ` - Class name : Voxels Module : romiscan . tasks . cl Description : Computes a volume from backprojection of 2D segmented images Default upstream tasks : - upstream_mask : Masks - upstream_colmap : Colmap Upstream task format : - upstream_mask : Fileset with grayscale images - upstream_colmap : Output of Colmap task Output fileset format : npz file with as many arrays as classes Parameters : - use_colmap_poses ( BoolParameter , default = True ): Either use precomputed camera poses or output from the Colmap task - voxel_size ( FloatParameter ): size of one side of voxels - type ( Parameter ): \"carving\" or \"averaging\" ( TODO : See 3D documentation ) - multiclass ( BoolParameter , default = False ): whether input data is single class or multiclass ( e . g as an output of Segmentation2D ) - log ( BoolParameter , default = True ), in the case of \"averaging\" type , whether to apply log when averaging values . Class name : PointCloud Module : romiscan . tasks . proc3d Description : Computes a point cloud from volumetric voxel data ( either single or multiclass ) Default upstream tasks : Voxels Upstream task format : npz file with as many 3D array as classes Output task format : single point cloud in ply . Metadata may include label name if multiclass . Class name : TriangleMesh Module : romiscan . tasks . proc3d Description : Triangulates input point cloud . Currently ignores class data and needs only one connected component . Default upstream tasks : PointCloud Upstream task format : ply file Output task format : ply triangle mesh file Class name : CurveSkeleton Module : romiscan . tasks . proc3d Description : Creates a 3D curve skeleton Default upstream tasks : TriangleMesh Upstream task format : ply triangle mesh Output task format : json with two entries \"points\" and \"lines\" ( TODO : precise ) Class name : TreeGraph Module : romiscan . tasks . arabidopsis Description : Creates a tree graph of the plant Default upstream tasks : CurveSkeleton Upstream task format : json Output task format : json ( TODO : precise ) Class name ; AnglesAndInternodes Module : romiscan . tasks . arabidopsis Description : Computes angles and internode Default upstream tasks : TreeGraph Upstream task format : json Output task format : json ( TODO : precise )","title":"Default task reference"},{"location":"Scanner/How-To/basics/#scanner-api-reference","text":"","title":"Scanner API reference"},{"location":"Scanner/How-To/basics/#objects","text":"/objects (GET): retrieve the list of obj files in the data folder that can be loaded. /load_object/<object_id> (GET) load the given object in the scene. Takes a translation vector as URL parameters ( dx , dy , dz )","title":"Objects"},{"location":"Scanner/How-To/basics/#classes","text":"/classes (GET): retrieve the list of classes.","title":"Classes"},{"location":"Scanner/How-To/basics/#backgrounds","text":"/backgrounds (GET): retrieve the list of hdr files in the hdri folder that can be loaded. /load_background/<background_id> (GET) load the given background in the scene.","title":"Backgrounds"},{"location":"Scanner/How-To/basics/#camera","text":"/camera_intrinsics (POST): set camera intrinsics. Keys: width , height , focal /camera_pose (POST): set camera pose. Keys: tx , ty , tz , rx , ry , rz","title":"Camera"},{"location":"Scanner/How-To/basics/#rendering","text":"/render (GET): gets the rendering of the scene /render_class/<class_id> (GET) renders the scene, with everything transparent except the given class TODO: missing endpoints httpie # Setup camera http - f post http : // localhost : 5000 / camera_intrinsics width = 1920 height = 1080 focal = 35 # Load arabidopsis_0 http get 'http://localhost:5000/load_object/arabidopsis_0.obj?dx=10&dy=20&dz=1' # Load \"old tree in the park\" background http get http : // 127 . 0 . 0 . 1 : 5000 / load_background / old_tree_in_city_park_8k . hdr # Move camera http - f post http : // localhost : 5000 / camera_pose tx =- 60 ty = 0 tz = 50 rx = 60 ry = 0 rz =- 90 # Render scene and download image http --download get http://localhost:5000/render # Render only leaves http --download get http://localhost:5000/render_class/Color_7","title":"Rendering"},{"location":"Scanner/How-To/hardware_scan/","text":"Info Do this only if you want to control the hardware and scan plants with the open-source scanner! Getting started Link To follows this guide you should have a conda or a Python venv , see here Install ROMI packages with pip : Link Install romidata : Link Since we will need an active database to performs the reconstruction, we install it as follows: pip install git+https://github.com/romi/data-storage.git@dev To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. Install lettucethink : Link pip install git+https://github.com/romi/lettucethink-python@dev Running scans Link Scan is the basic task for running a task with the scanner. A sample configuration file for the (real) scanner is as follows: [Scan.scanner] camera_firmware = \"sony_wifi\" cnc_firmware = \"grbl-v1.1\" gimbal_firmware = \"blgimbal\" [ Scan . scanner . scanner_args ] # These are the kwargs passed to the scanner constructor inverted = false [ Scan . scanner . camera_args ] # These are the kwargs passed to the camera constructor postview = true device_ip = \"10.0.2.66\" api_port = \"10000\" [ Scan . scanner . cnc_args ] # These are kwargs passed to the CNC constructor homing = true port = \"/dev/ttyUSB0\" [Scan.scanner.gimbal_args] port = \"/dev/ttyACM1\" has_tilt = false zero_pan = 145 [ Scan . scanner . camera_model ] # This is a precalibrated camera model width = 1616 height = 1080 id = 1 model = \"OPENCV\" params = [ 1120.72122223961, 1120.72122223961, 808.0, 540.0, 0.0007513494532588769, 0.0007513494532588769, 0.0, 0.0,] [ Scan . scanner . workspace ] # A volume containing the target scanned object x = [ 200, 600,] y = [ 200, 600,] z = [ -100, 300,] [ Scan . path ] # Example circular scan with 72 points: type = \"circular\" [Scan.path.args] num_points = 3 radius = 350 tilt = 0.45 # rad xc = 400 yc = 400 z = 0 [Scan.metadata] key = value # Any metadata you want to add to the scan To see available parameters for scanner, camera, CNC, check the romiscanner module. Todo Add a link to the romiscanner documentation! 1. Create a configuration file Link Create a file named scanner.toml with the following text, adjusting parameters as needed for the actual configuration of the scanner. Check the lettucethink documentation for additional information. Todo Add a link to the lettucethink documentation! 2. Launch the Scan task Link Assuming you have an active database, you can nom run a scan using romi_run_task : romi_run_task --config scanner.toml Scan /path/to/db/scan_id/ --local-scheduler where: /path/to/db must be an existing FSDB database scan_id must not already exist in the database. This will create the corresponding folder and fill it with images from the scan.","title":"Hardware scan"},{"location":"Scanner/How-To/hardware_scan/#getting-started","text":"To follows this guide you should have a conda or a Python venv , see here","title":"Getting started"},{"location":"Scanner/How-To/hardware_scan/#install-romi-packages-with-pip","text":"","title":"Install ROMI packages with pip:"},{"location":"Scanner/How-To/hardware_scan/#install-romidata","text":"Since we will need an active database to performs the reconstruction, we install it as follows: pip install git+https://github.com/romi/data-storage.git@dev To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database.","title":"Install romidata:"},{"location":"Scanner/How-To/hardware_scan/#install-lettucethink","text":"pip install git+https://github.com/romi/lettucethink-python@dev","title":"Install lettucethink:"},{"location":"Scanner/How-To/hardware_scan/#running-scans","text":"Scan is the basic task for running a task with the scanner. A sample configuration file for the (real) scanner is as follows: [Scan.scanner] camera_firmware = \"sony_wifi\" cnc_firmware = \"grbl-v1.1\" gimbal_firmware = \"blgimbal\" [ Scan . scanner . scanner_args ] # These are the kwargs passed to the scanner constructor inverted = false [ Scan . scanner . camera_args ] # These are the kwargs passed to the camera constructor postview = true device_ip = \"10.0.2.66\" api_port = \"10000\" [ Scan . scanner . cnc_args ] # These are kwargs passed to the CNC constructor homing = true port = \"/dev/ttyUSB0\" [Scan.scanner.gimbal_args] port = \"/dev/ttyACM1\" has_tilt = false zero_pan = 145 [ Scan . scanner . camera_model ] # This is a precalibrated camera model width = 1616 height = 1080 id = 1 model = \"OPENCV\" params = [ 1120.72122223961, 1120.72122223961, 808.0, 540.0, 0.0007513494532588769, 0.0007513494532588769, 0.0, 0.0,] [ Scan . scanner . workspace ] # A volume containing the target scanned object x = [ 200, 600,] y = [ 200, 600,] z = [ -100, 300,] [ Scan . path ] # Example circular scan with 72 points: type = \"circular\" [Scan.path.args] num_points = 3 radius = 350 tilt = 0.45 # rad xc = 400 yc = 400 z = 0 [Scan.metadata] key = value # Any metadata you want to add to the scan To see available parameters for scanner, camera, CNC, check the romiscanner module. Todo Add a link to the romiscanner documentation!","title":"Running scans"},{"location":"Scanner/How-To/hardware_scan/#1-create-a-configuration-file","text":"Create a file named scanner.toml with the following text, adjusting parameters as needed for the actual configuration of the scanner. Check the lettucethink documentation for additional information. Todo Add a link to the lettucethink documentation!","title":"1. Create a configuration file"},{"location":"Scanner/How-To/hardware_scan/#2-launch-the-scan-task","text":"Assuming you have an active database, you can nom run a scan using romi_run_task : romi_run_task --config scanner.toml Scan /path/to/db/scan_id/ --local-scheduler where: /path/to/db must be an existing FSDB database scan_id must not already exist in the database. This will create the corresponding folder and fill it with images from the scan.","title":"2. Launch the Scan task"},{"location":"Scanner/How-To/reconstruct_scan/","text":"How to use the plant reconstruction and analysis pipeline? Link Getting started Link To follows this guide you should have a conda or a Python venv , see here Install required dependencies colmap : Link Follow the procedure from the official documentation here . Make sure to use version 3.6. TODO: use COLMAP python build script to make conda package? Note: If you are using a conda environment, you can install ceres-solver dependency for COLMAP from the conda-forge channel: conda install ceres-solver -c conda-forge If you want to use NVIDIA for OpenCL in the processing pipeline, install pyopencl from source, and configure it to use OpenCL 1.2 (NVIDIA does not support the default OpenCL 2.0). First, make sure you have python headers installed, on ubuntu: apt install python3.7-dev Then install pyopencl from source, and configure it to use OpenCL 1.2: git clone https://github.com/inducer/pyopencl cd pyopencl git submodule update --init pip install pybind11 mako ./configure.py --cl-pretend-version = 1 .2 # NVIDIA has bad OpenCL support and only provides OpenCL 1.2 python setup.py install cd .. Troubeshooting: If you have an error [...] src/wrap_cl.hpp:57:10: fatal error: CL/cl.h [...] you are missing the OpenCL headers: sudo apt install opencl-headers If you have an error [...]compiler_compat/ld: cannot find -lOpenCL [...] : bash sudo apt install ocl-icd-libopencl1 Install ROMI packages with pip : Link This can be done both in the Python venv and the conda environment. Install data-storage sources: Link Since we will need an active database to performs the reconstruction, we install it as follows: pip install git+https://github.com/romi/data-storage.git@dev Install romiscanner sources: Link pip install git+https://github.com/romi/romiscanner Install cgal_bindings_skeletonization sources: Link pip install git+https://github.com/romi/cgal_bindings_skeletonization Note This takes some time since it has to download dependencies ( CGAL-0.5 & boost-1.72.0 ) and compile them. Install Scan3D sources: Link pip install git+https://github.com/romi/Scan3D@dev Warning If not using CUDA 10.*, you have to install the matching pytorch distribution. For example, for CUDA 9.2, use: pip install torch == 1 .4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html Reconstruction pipeline Link Test database Link To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database. Cleaning a dataset Link If you made a mess, had a failure or just want to start fresh with your dataset, no need to save a copy on the side, you can use the Clean task: romi_run_task --config Scan3D/config/original_pipe_0.toml Clean integration_tests/2019-02-01_10-56-33 --local-scheduler Here the config may use the [ Clean ] section where you can defines the force option: [Clean] force = true If true the Clean task will run silently, else in interactive mode. Geometric pipeline Link Real scan dataset Link The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on real dataset with: romi_run_task --config Scan3D/config/original_pipe_0.toml AnglesAndInternodes integration_tests/2019-02-01_10-56-33 --local-scheduler Note This example uses a real scan dataset from the test database. Virtual plant dataset Link The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on a virtual dataset with: romi_run_task --config Scan3D/config/original_pipe_0.toml AnglesAndInternodes integration_tests/arabidopsis_26 --local-scheduler Note This example uses a virtual scan dataset from the test database. Warning If you get something like this during the Voxel tasks: Choose platform: [0] <pyopencl.Platform 'NVIDIA CUDA' at 0x55d904d5af50> Choice [0]: that mean you need to specify the environment variable PYOPENCL_CTX='0' Machine Learning pipeline Link Real scan dataset Link The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on real dataset with: romi_run_task --config Scan3D/config/original_pipe_1.toml AnglesAndInternodes integration_tests/2019-02-01_10-56-33 --local-scheduler Note This example uses a real scan dataset from the test database. Virtual plant dataset Link The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on a virtual dataset with: romi_run_task --config Scan3D/config/original_pipe_1.toml AnglesAndInternodes integration_tests/arabidopsis_26 --local-scheduler Note This example uses a virtual scan dataset from the test database.","title":"Reconstruct scan"},{"location":"Scanner/How-To/reconstruct_scan/#how-to-use-the-plant-reconstruction-and-analysis-pipeline","text":"","title":"How to use the plant reconstruction and analysis pipeline?"},{"location":"Scanner/How-To/reconstruct_scan/#getting-started","text":"To follows this guide you should have a conda or a Python venv , see here","title":"Getting started"},{"location":"Scanner/How-To/reconstruct_scan/#install-required-dependencies-colmap","text":"Follow the procedure from the official documentation here . Make sure to use version 3.6. TODO: use COLMAP python build script to make conda package? Note: If you are using a conda environment, you can install ceres-solver dependency for COLMAP from the conda-forge channel: conda install ceres-solver -c conda-forge If you want to use NVIDIA for OpenCL in the processing pipeline, install pyopencl from source, and configure it to use OpenCL 1.2 (NVIDIA does not support the default OpenCL 2.0). First, make sure you have python headers installed, on ubuntu: apt install python3.7-dev Then install pyopencl from source, and configure it to use OpenCL 1.2: git clone https://github.com/inducer/pyopencl cd pyopencl git submodule update --init pip install pybind11 mako ./configure.py --cl-pretend-version = 1 .2 # NVIDIA has bad OpenCL support and only provides OpenCL 1.2 python setup.py install cd .. Troubeshooting: If you have an error [...] src/wrap_cl.hpp:57:10: fatal error: CL/cl.h [...] you are missing the OpenCL headers: sudo apt install opencl-headers If you have an error [...]compiler_compat/ld: cannot find -lOpenCL [...] : bash sudo apt install ocl-icd-libopencl1","title":"Install required dependencies colmap:"},{"location":"Scanner/How-To/reconstruct_scan/#install-romi-packages-with-pip","text":"This can be done both in the Python venv and the conda environment.","title":"Install ROMI packages with pip:"},{"location":"Scanner/How-To/reconstruct_scan/#install-data-storage-sources","text":"Since we will need an active database to performs the reconstruction, we install it as follows: pip install git+https://github.com/romi/data-storage.git@dev","title":"Install data-storage sources:"},{"location":"Scanner/How-To/reconstruct_scan/#install-romiscanner-sources","text":"pip install git+https://github.com/romi/romiscanner","title":"Install romiscanner sources:"},{"location":"Scanner/How-To/reconstruct_scan/#install-cgal_bindings_skeletonization-sources","text":"pip install git+https://github.com/romi/cgal_bindings_skeletonization Note This takes some time since it has to download dependencies ( CGAL-0.5 & boost-1.72.0 ) and compile them.","title":"Install cgal_bindings_skeletonization sources:"},{"location":"Scanner/How-To/reconstruct_scan/#install-scan3d-sources","text":"pip install git+https://github.com/romi/Scan3D@dev Warning If not using CUDA 10.*, you have to install the matching pytorch distribution. For example, for CUDA 9.2, use: pip install torch == 1 .4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html","title":"Install Scan3D sources:"},{"location":"Scanner/How-To/reconstruct_scan/#reconstruction-pipeline","text":"","title":"Reconstruction pipeline"},{"location":"Scanner/How-To/reconstruct_scan/#test-database","text":"To quickly create an example DB you can use: wget https://db.romi-project.eu/models/test_db.tar.gz tar -xf test_db.tar.gz This will create a integration_tests folder with a ready to use test database.","title":"Test database"},{"location":"Scanner/How-To/reconstruct_scan/#cleaning-a-dataset","text":"If you made a mess, had a failure or just want to start fresh with your dataset, no need to save a copy on the side, you can use the Clean task: romi_run_task --config Scan3D/config/original_pipe_0.toml Clean integration_tests/2019-02-01_10-56-33 --local-scheduler Here the config may use the [ Clean ] section where you can defines the force option: [Clean] force = true If true the Clean task will run silently, else in interactive mode.","title":"Cleaning a dataset"},{"location":"Scanner/How-To/reconstruct_scan/#geometric-pipeline","text":"","title":"Geometric pipeline"},{"location":"Scanner/How-To/reconstruct_scan/#real-scan-dataset","text":"The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on real dataset with: romi_run_task --config Scan3D/config/original_pipe_0.toml AnglesAndInternodes integration_tests/2019-02-01_10-56-33 --local-scheduler Note This example uses a real scan dataset from the test database.","title":"Real scan dataset"},{"location":"Scanner/How-To/reconstruct_scan/#virtual-plant-dataset","text":"The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on a virtual dataset with: romi_run_task --config Scan3D/config/original_pipe_0.toml AnglesAndInternodes integration_tests/arabidopsis_26 --local-scheduler Note This example uses a virtual scan dataset from the test database. Warning If you get something like this during the Voxel tasks: Choose platform: [0] <pyopencl.Platform 'NVIDIA CUDA' at 0x55d904d5af50> Choice [0]: that mean you need to specify the environment variable PYOPENCL_CTX='0'","title":"Virtual plant dataset"},{"location":"Scanner/How-To/reconstruct_scan/#machine-learning-pipeline","text":"","title":"Machine Learning pipeline"},{"location":"Scanner/How-To/reconstruct_scan/#real-scan-dataset_1","text":"The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on real dataset with: romi_run_task --config Scan3D/config/original_pipe_1.toml AnglesAndInternodes integration_tests/2019-02-01_10-56-33 --local-scheduler Note This example uses a real scan dataset from the test database.","title":"Real scan dataset"},{"location":"Scanner/How-To/reconstruct_scan/#virtual-plant-dataset_1","text":"The full geometric pipeline , ie. all the way to angles and internodes measurement, can be called on a virtual dataset with: romi_run_task --config Scan3D/config/original_pipe_1.toml AnglesAndInternodes integration_tests/arabidopsis_26 --local-scheduler Note This example uses a virtual scan dataset from the test database.","title":"Virtual plant dataset"},{"location":"Scanner/How-To/virtual_scan/","text":"How to generate a virtual scan? Link Getting started Link To follows this guide you should have a conda or a Python venv , see here Notice for using the virtual scanner Link If you want to use the virtual scanner, the blender and python version have to match. To obtain the python version bundled with your distribution of blender, type: blender - b -- python - expr \"import sys; print(sys.version)\" It will output something like: Blender 2 . 82 ( sub 7 ) ( hash 5 b416ffb848e built 2020 - 02 - 14 16 : 19 : 45 ) ALSA lib pcm_dmix . c : 1089 :( snd_pcm_dmix_open ) unable to open slave 3 . 8 . 1 ( default , Jan 22 2020 , 06 : 38 : 00 ) [ GCC 9 . 2 . 0 ] Blender quit In this case, this means python is using Python 3.8 and you should use python 3.8. The blender binaries found on their website bundle Python 3.8. In the following, we will assume that you are using conda environments. If not, adapt with corresponding virtualenv commands. Install ROMI packages & their dependencies: Link Install LPY (for virtual scans) Link If you're using python 3.7 and conda, just install lpy from conda: conda install -c conda-forge -c fredboudon openalea.lpy Note If you're using python 3.8, you must compile plantgl and lpy from sources. Basic usage Link The virtual scanner works like an HTTP server using Blender. First, make sure you have blender (>= 2.80) installed on your machine. Then, clone the directory and access it: git clone git@github.com:romi/blender_virtual_scanner.git cd blender_virtual_scanner You can obtain sample data for the scanner here, and put it in the data folder. wget https://db.romi-project.eu/models/arabidopsis_data.zip unzip arabidopsis_data.zip -d data To use custom data, it must consist in .obj file, in which each type of organ corresponds to a distinct mesh. This mesh must have a single material whose name is the name of the organ. The data dir must contain the obj and mtl files. Additionally, background HDRI files can be downloaded from (hdri haven)[ https://hdrihaven.com/ ]. Download .hdr files and put them in the hdri folder. To start the virtual scanner, run the following script in blender: blender [ scene/texture.blend ] -b -P scan.py It will start an HTTP server on port 5000 . Preparing data Link If you have 3D models with a single mesh Running a scan (with romiscan and lettucethink ) Link The virtual scanner is integrated in lettucethink-python, so that it can be used directly with the Scan task in romi_run_task . Here is a sample configuration for the virtual scanner creating 640x480 images with ground truth segmentation of organs. The server mentioned above must be running before running romi_run_task . [Scan.scanner] camera_firmware = \"virtual\" cnc_firmware = \"virtual\" gimbal_firmware = \"virtual\" id = \"virtual\" [Scan.path] id = \"virtual\" type = \"circular\" [Scan.scanner.scanner_args] inverted = false [Scan.scanner.camera_args] width = 640 height = 480 focal = 25 render_ground_truth = true load_object = \"arabidopsis_0.obj?dx=10&dy=10&dz=-5\" load_background = \"quarry_03_8k.hdr\" [Scan.scanner.cnc_args] [Scan.scanner.gimbal_args] [Scan.scanner.workspace] x = [ 200, 600,] y = [ 200, 600,] z = [ -100, 300,] [Scan.path.args] num_points = 10 radius = 100 tilt = 0.45 xc = 0 yc = 0 z = 50 Running the pipeline with ground-truth poses (without Colmap) Link To run the pipeline without colmap and use the virtual scanner poses as a ground truth, one must use set the following parameters: [Voxels] use_colmap_poses = false [Masks] upstream_task = Scan Then the pipeline can be run as usual and colmap will not be run. Testing the reconstruction pipeline on a virtual scan Link To test the plant reconstruction pipeline on an example virtual scan ( arabidopsis_26 ): romi_run_task --config Scan3D/config/original_pipe_0.toml PointCloud integration_tests/arabidopsis_26 --local-scheduler This should process all dependencies to obtain a segmented \"PointCloud.ply\" !","title":"Virtual scan"},{"location":"Scanner/How-To/virtual_scan/#how-to-generate-a-virtual-scan","text":"","title":"How to generate a virtual scan?"},{"location":"Scanner/How-To/virtual_scan/#getting-started","text":"To follows this guide you should have a conda or a Python venv , see here","title":"Getting started"},{"location":"Scanner/How-To/virtual_scan/#notice-for-using-the-virtual-scanner","text":"If you want to use the virtual scanner, the blender and python version have to match. To obtain the python version bundled with your distribution of blender, type: blender - b -- python - expr \"import sys; print(sys.version)\" It will output something like: Blender 2 . 82 ( sub 7 ) ( hash 5 b416ffb848e built 2020 - 02 - 14 16 : 19 : 45 ) ALSA lib pcm_dmix . c : 1089 :( snd_pcm_dmix_open ) unable to open slave 3 . 8 . 1 ( default , Jan 22 2020 , 06 : 38 : 00 ) [ GCC 9 . 2 . 0 ] Blender quit In this case, this means python is using Python 3.8 and you should use python 3.8. The blender binaries found on their website bundle Python 3.8. In the following, we will assume that you are using conda environments. If not, adapt with corresponding virtualenv commands.","title":"Notice for using the virtual scanner"},{"location":"Scanner/How-To/virtual_scan/#install-romi-packages-their-dependencies","text":"","title":"Install ROMI packages &amp; their dependencies:"},{"location":"Scanner/How-To/virtual_scan/#install-lpy-for-virtual-scans","text":"If you're using python 3.7 and conda, just install lpy from conda: conda install -c conda-forge -c fredboudon openalea.lpy Note If you're using python 3.8, you must compile plantgl and lpy from sources.","title":"Install LPY (for virtual scans)"},{"location":"Scanner/How-To/virtual_scan/#basic-usage","text":"The virtual scanner works like an HTTP server using Blender. First, make sure you have blender (>= 2.80) installed on your machine. Then, clone the directory and access it: git clone git@github.com:romi/blender_virtual_scanner.git cd blender_virtual_scanner You can obtain sample data for the scanner here, and put it in the data folder. wget https://db.romi-project.eu/models/arabidopsis_data.zip unzip arabidopsis_data.zip -d data To use custom data, it must consist in .obj file, in which each type of organ corresponds to a distinct mesh. This mesh must have a single material whose name is the name of the organ. The data dir must contain the obj and mtl files. Additionally, background HDRI files can be downloaded from (hdri haven)[ https://hdrihaven.com/ ]. Download .hdr files and put them in the hdri folder. To start the virtual scanner, run the following script in blender: blender [ scene/texture.blend ] -b -P scan.py It will start an HTTP server on port 5000 .","title":"Basic usage"},{"location":"Scanner/How-To/virtual_scan/#preparing-data","text":"If you have 3D models with a single mesh","title":"Preparing data"},{"location":"Scanner/How-To/virtual_scan/#running-a-scan-with-romiscan-and-lettucethink","text":"The virtual scanner is integrated in lettucethink-python, so that it can be used directly with the Scan task in romi_run_task . Here is a sample configuration for the virtual scanner creating 640x480 images with ground truth segmentation of organs. The server mentioned above must be running before running romi_run_task . [Scan.scanner] camera_firmware = \"virtual\" cnc_firmware = \"virtual\" gimbal_firmware = \"virtual\" id = \"virtual\" [Scan.path] id = \"virtual\" type = \"circular\" [Scan.scanner.scanner_args] inverted = false [Scan.scanner.camera_args] width = 640 height = 480 focal = 25 render_ground_truth = true load_object = \"arabidopsis_0.obj?dx=10&dy=10&dz=-5\" load_background = \"quarry_03_8k.hdr\" [Scan.scanner.cnc_args] [Scan.scanner.gimbal_args] [Scan.scanner.workspace] x = [ 200, 600,] y = [ 200, 600,] z = [ -100, 300,] [Scan.path.args] num_points = 10 radius = 100 tilt = 0.45 xc = 0 yc = 0 z = 50","title":"Running a scan (with romiscan and lettucethink)"},{"location":"Scanner/How-To/virtual_scan/#running-the-pipeline-with-ground-truth-poses-without-colmap","text":"To run the pipeline without colmap and use the virtual scanner poses as a ground truth, one must use set the following parameters: [Voxels] use_colmap_poses = false [Masks] upstream_task = Scan Then the pipeline can be run as usual and colmap will not be run.","title":"Running the pipeline with ground-truth poses (without Colmap)"},{"location":"Scanner/How-To/virtual_scan/#testing-the-reconstruction-pipeline-on-a-virtual-scan","text":"To test the plant reconstruction pipeline on an example virtual scan ( arabidopsis_26 ): romi_run_task --config Scan3D/config/original_pipe_0.toml PointCloud integration_tests/arabidopsis_26 --local-scheduler This should process all dependencies to obtain a segmented \"PointCloud.ply\" !","title":"Testing the reconstruction pipeline on a virtual scan"},{"location":"Scanner/How-To/visualizer/","text":"How to use the ROMI plantviewer? Link Ready to run docker image Link To use a ready to run docker image pointing toward the db.romi-project.eu , look here . Note This requires docker-ce to be installed on your machine. Source install Link To follows this guide you should have a conda or a Python venv , see here Pre-requisite Link The plantviewer relies on: node npm To clone the git repository, you will need: git ca-certificates Start with these system dependencies: sudo apt git ca-certificates Install node and npm , on ubuntu: sudo apt install npm The packaged version ot npm is probably out of date (require npm>=5 ), to update it: npm install npm@latest -g Install ROMI packages & their dependencies: Link Clone the visualizer git repository : git clone https://github.com/romi/3d-plantviewer.git cd 3d-plantviewer Install node packages and build the pages: npm install Running a development server for the visualizer Link Set the DB location using the DB_LOCATION environment variable and launch the flask server: export DB_LOCATION = /path/to/the/db romi_scanner_rest_api Finally, start the frontend visualization server (from 3d-plantviewer/ folder): npm start You should now be able to access the visualizer on http://localhost:3000 . Note You need to add a file .env.local at project's root to set the API URL: REACT_APP_API_URL='{`API URL}' Without this, the app will use: http://localhost:5000 which is the default for romi_scanner_rest_api . Running a production server for the visualizer Link Warning This is not tested yet! Visualizer API reference Link Warning Not too many details here, I like it!","title":"Visualizer"},{"location":"Scanner/How-To/visualizer/#how-to-use-the-romi-plantviewer","text":"","title":"How to use the ROMI plantviewer?"},{"location":"Scanner/How-To/visualizer/#ready-to-run-docker-image","text":"To use a ready to run docker image pointing toward the db.romi-project.eu , look here . Note This requires docker-ce to be installed on your machine.","title":"Ready to run docker image"},{"location":"Scanner/How-To/visualizer/#source-install","text":"To follows this guide you should have a conda or a Python venv , see here","title":"Source install"},{"location":"Scanner/How-To/visualizer/#pre-requisite","text":"The plantviewer relies on: node npm To clone the git repository, you will need: git ca-certificates Start with these system dependencies: sudo apt git ca-certificates Install node and npm , on ubuntu: sudo apt install npm The packaged version ot npm is probably out of date (require npm>=5 ), to update it: npm install npm@latest -g","title":"Pre-requisite"},{"location":"Scanner/How-To/visualizer/#install-romi-packages-their-dependencies","text":"Clone the visualizer git repository : git clone https://github.com/romi/3d-plantviewer.git cd 3d-plantviewer Install node packages and build the pages: npm install","title":"Install ROMI packages &amp; their dependencies:"},{"location":"Scanner/How-To/visualizer/#running-a-development-server-for-the-visualizer","text":"Set the DB location using the DB_LOCATION environment variable and launch the flask server: export DB_LOCATION = /path/to/the/db romi_scanner_rest_api Finally, start the frontend visualization server (from 3d-plantviewer/ folder): npm start You should now be able to access the visualizer on http://localhost:3000 . Note You need to add a file .env.local at project's root to set the API URL: REACT_APP_API_URL='{`API URL}' Without this, the app will use: http://localhost:5000 which is the default for romi_scanner_rest_api .","title":"Running a development server for the visualizer"},{"location":"Scanner/How-To/visualizer/#running-a-production-server-for-the-visualizer","text":"Warning This is not tested yet!","title":"Running a production server for the visualizer"},{"location":"Scanner/How-To/visualizer/#visualizer-api-reference","text":"Warning Not too many details here, I like it!","title":"Visualizer API reference"}]}